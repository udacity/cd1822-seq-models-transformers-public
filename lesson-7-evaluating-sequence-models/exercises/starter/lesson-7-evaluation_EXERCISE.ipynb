{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6014004",
   "metadata": {},
   "source": [
    "# Lesson 7 Exercise: Evaluate and Analyze Q&A Model Performance\n",
    "\n",
    "**Estimated Time:** 17 minutes\n",
    "\n",
    "## Scenario\n",
    "\n",
    "Your Q&A model achieves **78% F1** but management sees user complaints about wrong answers.\n",
    "\n",
    "**Questions to answer:**\n",
    "- Are mistakes on long contexts?\n",
    "- Unanswerable questions?\n",
    "- Specific question types?\n",
    "\n",
    "**Goal:** Perform systematic error analysis to prioritize improvements!\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fdb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install transformers datasets evaluate pandas matplotlib torch\n",
    "\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99324ed8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Generate Predictions and Compute Metrics (6 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261a0cc",
   "metadata": {},
   "source": [
    "## Step 1: Load BERT Q&A Model\n",
    "\n",
    "We'll use the same BERT from Lesson 6, now fine-tuned for Q&A!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8227a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load BERT Q&A model using HuggingFace pipeline\n",
    "# Hint: Use pipeline(\"question-answering\") with model=\"deepset/bert-base-cased-squad2\"\n",
    "# Hint: Check if GPU is available with torch.cuda.is_available() and set device accordingly\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"Loading BERT Q&A model...\")\n",
    "print(\"(First run will download ~400MB model)\\n\")\n",
    "\n",
    "# TODO: Complete this section\n",
    "device = None  # TODO: Set to 0 if GPU available, else -1\n",
    "qa_pipeline = None  # TODO: Load the pipeline\n",
    "\n",
    "print(f\"âœ“ BERT Q&A model loaded on {'GPU' if device == 0 else 'CPU'}\")\n",
    "print(\"  This is the same BERT architecture from Lesson 6!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125cc4e",
   "metadata": {},
   "source": [
    "## Step 2: Load SQuAD 2.0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load SQuAD 2.0 validation set\n",
    "# Hint: Use load_dataset(\"squad_v2\", split=\"validation[:250]\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading SQuAD 2.0 validation set...\")\n",
    "dataset = None  # TODO: Load the dataset\n",
    "print(f\"âœ“ Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample question:\")\n",
    "print(f\"Q: {dataset[0]['question']}\")\n",
    "print(f\"Context: {dataset[0]['context'][:150]}...\")\n",
    "print(f\"Answer: {dataset[0]['answers']['text'][0] if dataset[0]['answers']['text'] else '[Unanswerable]'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6189b5f",
   "metadata": {},
   "source": [
    "## Step 3: Generate Predictions with BERT\n",
    "\n",
    "We'll get top-5 predictions for ranking metrics later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement prediction generation with BERT\n",
    "# Hint: For each example, call qa_pipeline with question, context, and top_k=5\n",
    "# Hint: Handle both single results and list of results (when top_k > 1)\n",
    "# Hint: Store prediction_text, score, and top_k_predictions\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Helper functions for metrics\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Normalize answer text.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return float(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(truth_tokens)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "print(\"Generating predictions with BERT...\")\n",
    "print(\"(This will take 2-3 minutes on CPU)\\n\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# TODO: Loop through dataset and generate predictions\n",
    "# TODO: For each example, call qa_pipeline with top_k=5\n",
    "# TODO: Store all results in predictions list\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(predictions)} predictions!\")\n",
    "\n",
    "# Show statistics\n",
    "total = len(predictions)\n",
    "correct = sum(1 for p in predictions if p['em_score'] == 1.0)\n",
    "errors = sum(1 for p in predictions if p['f1_score'] < 1.0)\n",
    "print(f\"\\nQuick stats:\")\n",
    "print(f\"  Correct (EM=100%): {correct} ({100*correct/total:.1f}%)\")\n",
    "print(f\"  Errors (F1<100%):  {errors} ({100*errors/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fad98c",
   "metadata": {},
   "source": [
    "## Step 4: Compute Ranking Metrics (P@K, R@K, MRR)\n",
    "\n",
    "Since we have top-5 predictions, let's compute retrieval metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement ranking metric functions\n",
    "# Hint: Implement compute_precision_at_k, compute_recall_at_k, compute_reciprocal_rank\n",
    "# Hint: Precision@K = what fraction of top-K are correct\n",
    "# Hint: Recall@K = what fraction of correct answers appear in top-K\n",
    "# Hint: MRR = 1/rank of first correct answer\n",
    "\n",
    "def compute_precision_at_k(ranked_predictions, ground_truths, k=3):\n",
    "    \"\"\"TODO: What fraction of top-K are correct?\"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_recall_at_k(ranked_predictions, ground_truths, k=3):\n",
    "    \"\"\"TODO: What fraction of correct answers appear in top-K?\"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_reciprocal_rank(ranked_predictions, ground_truths):\n",
    "    \"\"\"TODO: Return 1/rank of first correct answer.\"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO: Compute ranking metrics for answerable questions\n",
    "# TODO: Loop through answerable predictions and compute P@3, R@3, MRR\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANKING METRICS (Top-5 Predictions)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Precision@3:  {avg_precision_at_3:.2%}\")\n",
    "print(f\"Recall@3:     {avg_recall_at_3:.2%}\")\n",
    "print(f\"MRR:          {avg_mrr:.3f}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ’¡ These metrics measure retrieval quality:\")\n",
    "print(\"   - P@3: What % of top-3 are correct (quality)\")\n",
    "print(\"   - R@3: Do we get all correct answers in top-3 (coverage)\")\n",
    "print(\"   - MRR: How high is the first correct answer ranked (UX)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6d32d",
   "metadata": {},
   "source": [
    "## Step 5: Use HuggingFace Evaluate for EM and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800339de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use official SQuAD v2 metric from evaluate library\n",
    "# Hint: Load \"squad_v2\" metric with evaluate.load()\n",
    "# Hint: Format predictions and references in SQuAD v2 format\n",
    "# Hint: Call squad_metric.compute() with predictions and references\n",
    "\n",
    "import evaluate\n",
    "\n",
    "squad_metric = None  # TODO: Load the squad_v2 metric\n",
    "\n",
    "# TODO: Format predictions and references for the official metric\n",
    "formatted_predictions = []  # TODO: Create list of formatted predictions\n",
    "formatted_references = []   # TODO: Create list of formatted references\n",
    "\n",
    "results = None  # TODO: Compute metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL METRICS (Official SQuAD v2)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Exact Match (EM): {results['exact']:.2f}%\")\n",
    "print(f\"F1 Score:         {results['f1']:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4ee77",
   "metadata": {},
   "source": [
    "## Step 6: Create Comprehensive Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create summary DataFrame with all metrics\n",
    "# Hint: Include EM, F1, P@3, R@3, MRR, and number of examples\n",
    "\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Exact Match',\n",
    "        'F1 Score',\n",
    "        'Precision@3',\n",
    "        'Recall@3',\n",
    "        'MRR',\n",
    "        'Number of Examples'\n",
    "    ],\n",
    "    'Score': [\n",
    "        # TODO: Fill in metric values\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nComprehensive Metrics Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc3d87",
   "metadata": {},
   "source": [
    "## Step 7: Metrics by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute metrics separately for answerable vs unanswerable questions\n",
    "# Hint: Filter predictions by is_impossible field\n",
    "# Hint: Format and compute metrics for each group separately\n",
    "\n",
    "answerable_preds = None  # TODO: Filter answerable\n",
    "unanswerable_preds = None  # TODO: Filter unanswerable\n",
    "\n",
    "# TODO: Format and compute answerable_results\n",
    "answerable_results = None\n",
    "\n",
    "# TODO: Format and compute unanswerable_results\n",
    "unanswerable_results = None\n",
    "\n",
    "# Print breakdown\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METRICS BY QUESTION TYPE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nAnswerable Questions ({len(answerable_preds)} examples):\")\n",
    "print(f\"  Exact Match: {answerable_results['exact']:.2f}%\")\n",
    "print(f\"  F1 Score:    {answerable_results['f1']:.2f}%\")\n",
    "print(f\"  P@3:         {avg_precision_at_3:.2%}\")\n",
    "print(f\"  R@3:         {avg_recall_at_3:.2%}\")\n",
    "print(f\"  MRR:         {avg_mrr:.3f}\")\n",
    "print(f\"\\nUnanswerable Questions ({len(unanswerable_preds)} examples):\")\n",
    "print(f\"  Exact Match: {unanswerable_results['exact']:.2f}%\")\n",
    "print(f\"  F1 Score:    {unanswerable_results['f1']:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ’¡ Observation: Model performs worse on unanswerable questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ade677",
   "metadata": {},
   "source": [
    "## Checkpoint: Part A Complete! âœ“\n",
    "\n",
    "We now have:\n",
    "- âœ“ Real BERT predictions generated\n",
    "- âœ“ EM and F1 metrics computed\n",
    "- âœ“ Ranking metrics (P@3, R@3, MRR) computed\n",
    "- âœ“ Breakdown by question type\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9502fe",
   "metadata": {},
   "source": [
    "# Part B: Categorize and Visualize Errors (8 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f4d73",
   "metadata": {},
   "source": [
    "## Step 8: Filter Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter predictions where F1 < 1.0\n",
    "errors = None  # TODO: Filter for errors\n",
    "\n",
    "print(f\"Found {len(errors)} errors (F1 < 100%)\")\n",
    "print(f\"This is {100*len(errors)/len(predictions):.1f}% of all predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ea66c",
   "metadata": {},
   "source": [
    "## Step 9: Sample 30 Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly sample 30 errors for analysis\n",
    "# Hint: Use random.sample(errors, min(30, len(errors)))\n",
    "\n",
    "n_to_analyze = 30\n",
    "sample_errors = None  # TODO: Sample errors\n",
    "\n",
    "print(f\"Sampled {len(sample_errors)} errors for manual categorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b84912",
   "metadata": {},
   "source": [
    "## Step 10: Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_error(error, index):\n",
    "    \"\"\"Display error for inspection.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ERROR #{index + 1}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Question: {error['question']}\")\n",
    "    print(f\"\\nContext: {error['context'][:200]}...\")\n",
    "    print(f\"\\nModel Predicted:  '{error['prediction_text']}'\")\n",
    "    print(f\"Ground Truth:      '{error['ground_truth']}'\")\n",
    "    print(f\"\\nF1 Score: {error['f1_score']:.2%}\")\n",
    "    print(f\"Question Type: {'Unanswerable' if error['is_impossible'] else 'Answerable'}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Test it\n",
    "display_error(sample_errors[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c5dec",
   "metadata": {},
   "source": [
    "## Step 11: Categorize Errors\n",
    "\n",
    "Based on inspection of real BERT errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Categorize errors based on patterns\n",
    "# Hint: Categories could be: unanswerable_error, hallucination, wrong_span, partial_answer\n",
    "# Hint: Examine each error's characteristics to assign a category\n",
    "\n",
    "error_categories = []  # TODO: Assign categories to each error\n",
    "\n",
    "print(f\"âœ“ Categorized {len(error_categories)} errors\")\n",
    "print(f\"\\nCategories assigned:\")\n",
    "for cat, count in Counter(error_categories).items():\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b8037",
   "metadata": {},
   "source": [
    "## Step 12: Count by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69673c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Count error categories and calculate percentages\n",
    "category_counts = None  # TODO: Count categories\n",
    "total_categorized = len(error_categories)\n",
    "\n",
    "category_percentages = None  # TODO: Calculate percentages\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ERROR DISTRIBUTION BY CATEGORY\")\n",
    "print(\"=\"*70)\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = category_percentages[category]\n",
    "    print(f\"{category:20s}: {count:3d} ({pct:.1f}%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a064347",
   "metadata": {},
   "source": [
    "## Step 13: Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86902687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create bar plot of error distribution\n",
    "# Hint: Use plt.bar() with categories and counts\n",
    "# Hint: Add value labels on bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = None  # TODO: Get categories\n",
    "counts = None  # TODO: Get counts\n",
    "\n",
    "# TODO: Create bar plot\n",
    "ax.set_xlabel('Error Category', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Error Distribution by Category (30 Sampled Errors)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5029f1e",
   "metadata": {},
   "source": [
    "## Step 14: Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save categorized errors to CSV\n",
    "# Hint: Create list of dicts with id, question, prediction, ground_truth, f1_score, is_impossible, error_type\n",
    "# Hint: Convert to DataFrame and save with to_csv()\n",
    "\n",
    "categorized_data = []  # TODO: Build categorized data list\n",
    "\n",
    "categorized_df = pd.DataFrame(categorized_data)\n",
    "categorized_df.to_csv('categorized_errors.csv', index=False)\n",
    "\n",
    "print(\"âœ“ Saved categorized errors to categorized_errors.csv\")\n",
    "print(f\"âœ“ Saved {len(categorized_df)} categorized errors\")\n",
    "print(\"\\n\" + categorized_df.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754701b4",
   "metadata": {},
   "source": [
    "## Checkpoint: Part B Complete! âœ“\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03891cf5",
   "metadata": {},
   "source": [
    "# Part C: Identify Patterns and Document Findings (3 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c37b7",
   "metadata": {},
   "source": [
    "## Step 15: Identify Top 2 Error Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e986b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the 2 most common error types\n",
    "# Hint: Use category_counts.most_common(2)\n",
    "\n",
    "top_2_errors = None  # TODO: Get top 2\n",
    "\n",
    "print(\"Top 2 Error Types:\")\n",
    "for i, (category, count) in enumerate(top_2_errors, 1):\n",
    "    pct = 100 * count / total_categorized\n",
    "    print(f\"{i}. {category}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae06ee",
   "metadata": {},
   "source": [
    "## Step 16-17: Inspect Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077203c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select 3 examples of each top error type\n",
    "# Hint: Filter sample_errors by error_categories to get examples of each type\n",
    "\n",
    "top_1_category = None  # TODO: Get top 1 category\n",
    "top_2_category = None  # TODO: Get top 2 category\n",
    "\n",
    "top_1_examples = []  # TODO: Get 3 examples of top 1\n",
    "top_2_examples = []  # TODO: Get 3 examples of top 2\n",
    "\n",
    "print(f\"\\nTop error type 1: {top_1_category}\")\n",
    "print(f\"Selected {len(top_1_examples)} examples\")\n",
    "\n",
    "print(f\"\\nTop error type 2: {top_2_category}\")\n",
    "print(f\"Selected {len(top_2_examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display examples from each top error type\n",
    "# Hint: Loop through top_1_examples and top_2_examples\n",
    "# Hint: Print question, context, prediction, ground truth, F1 score for each\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOP ERROR TYPE 1: {top_1_category.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TODO: Display top_1_examples\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOP ERROR TYPE 2: {top_2_category.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TODO: Display top_2_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a7694",
   "metadata": {},
   "source": [
    "## Step 18: Document Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9febd42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### FINDINGS:\n",
    "\n",
    "**1. Top 2 Error Types:**\n",
    "- TODO: What are the top 2 error types you found?\n",
    "- TODO: What percentage of errors does each represent?\n",
    "\n",
    "**2. Patterns Observed:**\n",
    "\n",
    "*Top Error Type 1:*\n",
    "- TODO: After inspecting 3 examples, what patterns do you notice?\n",
    "- TODO: When/why does this error occur?\n",
    "- TODO: What contextual clues cause the model to make this mistake?\n",
    "\n",
    "*Top Error Type 2:*\n",
    "- TODO: After inspecting 3 examples, what patterns do you notice?\n",
    "- TODO: When/why does this error occur?\n",
    "- TODO: What could be improved to prevent this error?\n",
    "\n",
    "**3. Proposed Improvement:**\n",
    "\n",
    "Based on your top error type:\n",
    "- TODO: What specific change would you make to reduce this error?\n",
    "- TODO: Would you add more training data, change the model, or modify preprocessing?\n",
    "- TODO: What's your confidence that this would improve performance?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57380be7",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Exercise Complete!\n",
    "\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**Metrics + Error Analysis = Complete Evaluation**\n",
    "\n",
    "You now understand:\n",
    "- WHAT the performance is (F1, EM)\n",
    "- HOW retrieval quality looks (P@K, R@K, MRR)\n",
    "- WHY failures occur (error categorization)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
