{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 Demo: Implementing Evaluation Metrics for Question Answering\n",
    "\n",
    "**Estimated Time:** 7 minutes\n",
    "\n",
    "## Connection to Lesson 6\n",
    "\n",
    "**Remember Lesson 6?** We explored BERT's attention patterns and saw how Transformers understand context.\n",
    "\n",
    "**Lesson 7:** Now we'll use that same BERT architecture for question answering and learn how to **evaluate if the answers are actually correct!**\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario: Q&A System Quality Assurance\n",
    "\n",
    "Your Q&A system retrieves and generates answers that \"sound right\" but are often wrong in subtle ways. Management wants **quantitative metrics** to track improvement and compare model versions.\n",
    "\n",
    "**Problem:** Training loss says the model is learning, but does it give correct answers?\n",
    "\n",
    "**Solution:** Specialized evaluation metrics for question answering!\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Learn\n",
    "\n",
    "1. âœ… **Exact Match (EM)** - Binary score for perfect matches\n",
    "2. âœ… **F1 Score** - Token-level overlap (gives partial credit)\n",
    "3. âœ… **Precision@K / Recall@K** - Quality and coverage of ranked results\n",
    "4. âœ… **MRR** - Ranking effectiveness for user experience\n",
    "5. âœ… **When to use each metric**\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if needed)\n",
    "# !pip install datasets transformers torch evaluate matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from data import load_squad_v2, get_statistics, get_demo_examples\n",
    "from predictions import load_bert_qa_model, generate_predictions, show_prediction_examples\n",
    "from metrics import (\n",
    "    compute_exact_match,\n",
    "    compute_f1,\n",
    "    compute_em_for_dataset,\n",
    "    compute_f1_for_dataset,\n",
    "    compute_metrics_by_type,\n",
    "    print_metrics_summary,\n",
    "    normalize_answer\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Load BERT Q&A Model (1 minute)\n",
    "\n",
    "**From Lesson 6:** We loaded `bert-base-uncased` and explored attention.\n",
    "\n",
    "**Now:** We'll load a BERT model **fine-tuned on SQuAD** for question answering. Same architecture, specialized for Q&A!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Q&A model: deepset/bert-base-cased-squad2\n",
      "(This is the same BERT from Lesson 6, fine-tuned for Q&A!)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92103683703b40e5b4bdf6af2221160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f7e7efe4c1407aa17c75d365f1666c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4703bd2537da491788dd0f24d05f531f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1f2d794b224394bf36baef3f080756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6d0ea2ab414cca864ba8e34cf8cc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully!\n",
      "  Device: CPU\n",
      "\n",
      "ðŸ’¡ Key Point:\n",
      "   This is the SAME BERT architecture from Lesson 6.\n",
      "   Just fine-tuned on question-answering data (SQuAD).\n",
      "   The attention mechanisms we visualized in L6 are working here too!\n"
     ]
    }
   ],
   "source": [
    "# Load BERT Q&A model (fine-tuned on SQuAD 2.0)\n",
    "# This is the same BERT from Lesson 6!\n",
    "qa_model = load_bert_qa_model()\n",
    "\n",
    "print(\"ðŸ’¡ Key Point:\")\n",
    "print(\"   This is the SAME BERT architecture from Lesson 6.\")\n",
    "print(\"   Just fine-tuned on question-answering data (SQuAD).\")\n",
    "print(\"   The attention mechanisms we visualized in L6 are working here too!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD 2.0 dataset...\n",
      "\n",
      "Loading SQuAD 2.0 validation split...\n",
      "Requesting 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9175e71bf4429c9182df3bdc65d1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1017e5143d4fd892c22b16fccf4e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "squad_v2/train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3868d116f36c401cbf00b6abeb087e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "squad_v2/validation-00000-of-00001.parqu(â€¦):   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b357cd0960264f80a6c106e0d8028d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d945b91370a42e899778c1ca45d0c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 11873 total examples from SQuAD 2.0\n",
      "\n",
      "\n",
      "âœ“ Loaded 100 questions\n",
      "  Answerable:   53 (53.0%)\n",
      "  Unanswerable: 47 (47.0%)\n",
      "\n",
      "ðŸ’¡ SQuAD 2.0 includes unanswerable questions - model should return empty string!\n"
     ]
    }
   ],
   "source": [
    "# Load SQuAD 2.0 validation set\n",
    "print(\"Loading SQuAD 2.0 dataset...\\n\")\n",
    "squad_examples = load_squad_v2(n_samples=100)  # Small subset for demo speed\n",
    "\n",
    "# Get statistics\n",
    "stats = get_statistics(squad_examples)\n",
    "print(f\"\\nâœ“ Loaded {stats['total']} questions\")\n",
    "print(f\"  Answerable:   {stats['answerable']} ({stats['answerable_percent']:.1f}%)\")\n",
    "print(f\"  Unanswerable: {stats['unanswerable']} ({stats['unanswerable_percent']:.1f}%)\")\n",
    "print(f\"\\nðŸ’¡ SQuAD 2.0 includes unanswerable questions - model should return empty string!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Generate Predictions with BERT (1 minute)\n",
    "\n",
    "Let's see BERT answer questions in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 100 examples...\n",
      "(This may take 1-2 minutes on CPU)\n",
      "\n",
      "  Processed 20/100 examples...\n",
      "  Processed 40/100 examples...\n",
      "  Processed 60/100 examples...\n",
      "  Processed 80/100 examples...\n",
      "  Processed 100/100 examples...\n",
      "\n",
      "âœ“ Generated 100 predictions!\n",
      "================================================================================\n",
      "SAMPLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "1. Question: What pathway that plays a role in immune response to viruses is present in all eukaryotes?\n",
      "   BERT Predicted: 'RNA interference pathway'\n",
      "   Ground Truth:   'RNA interference pathway'\n",
      "   Confidence:     0.474\n",
      "   Type:           Answerable\n",
      "   Result:         âœ“ EXACT MATCH\n",
      "\n",
      "2. Question: What was developed for the Air Force \n",
      "   BERT Predicted: 'into survivable communications networks'\n",
      "   Ground Truth:   'survivable communications networks'\n",
      "   Confidence:     0.118\n",
      "   Type:           Answerable\n",
      "   Result:         ~ PARTIAL (check F1)\n",
      "\n",
      "3. Question: Why are the small lakes in the parks emptied before winter?\n",
      "   BERT Predicted: 'to clean them of plants and sediments'\n",
      "   Ground Truth:   'to clean them'\n",
      "   Confidence:     0.688\n",
      "   Type:           Answerable\n",
      "   Result:         ~ PARTIAL (check F1)\n",
      "\n",
      "4. Question: How many days did the Warsaw Uprising last?\n",
      "   BERT Predicted: '63 days'\n",
      "   Ground Truth:   '63 days'\n",
      "   Confidence:     0.483\n",
      "   Type:           Answerable\n",
      "   Result:         âœ“ EXACT MATCH\n",
      "\n",
      "5. Question: By what main attribute are computational problems classified utilizing computational complexity theory? \n",
      "   BERT Predicted: 'inherent difficulty'\n",
      "   Ground Truth:   'inherent difficulty'\n",
      "   Confidence:     0.747\n",
      "   Type:           Answerable\n",
      "   Result:         âœ“ EXACT MATCH\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for our examples\n",
    "predictions = generate_predictions(qa_model, squad_examples, max_samples=100)\n",
    "\n",
    "# Show some examples\n",
    "show_prediction_examples(predictions, n_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Some predictions look perfect, others are close but not exact.\n",
    "\n",
    "**Question:** How do we measure if these are \"correct\"? That's where metrics come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Exact Match (EM) - The Strict Metric (1 minute)\n",
    "\n",
    "**Exact Match:** 100% if prediction exactly matches ground truth (after normalization), otherwise 0%.\n",
    "\n",
    "**Normalization:**\n",
    "- Lowercase\n",
    "- Remove punctuation\n",
    "- Remove articles (a, an, the)\n",
    "- Remove extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Examples:\n",
      "============================================================\n",
      "\n",
      "Prediction: 'The answer' â†’ 'answer'\n",
      "Truth:      'answer' â†’ 'answer'\n",
      "EM Score:   100% âœ“\n",
      "\n",
      "Prediction: 'New York City' â†’ 'new york city'\n",
      "Truth:      'New York' â†’ 'new york'\n",
      "EM Score:   0% âœ—\n",
      "\n",
      "Prediction: 'Paris!' â†’ 'paris'\n",
      "Truth:      'Paris' â†’ 'paris'\n",
      "EM Score:   100% âœ“\n",
      "\n",
      "ðŸ’¡ Key Insight:\n",
      "   Normalization fixes 'The answer' vs 'answer' â†’ EM = 100%\n",
      "   But 'New York City' vs 'New York' â†’ Still EM = 0% (too strict!)\n"
     ]
    }
   ],
   "source": [
    "# Example: Why normalization matters\n",
    "print(\"Normalization Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "examples = [\n",
    "    (\"The answer\", \"answer\"),\n",
    "    (\"New York City\", \"New York\"),\n",
    "    (\"Paris!\", \"Paris\")\n",
    "]\n",
    "\n",
    "for pred, truth in examples:\n",
    "    pred_norm = normalize_answer(pred)\n",
    "    truth_norm = normalize_answer(truth)\n",
    "    em = compute_exact_match(pred, truth)\n",
    "    \n",
    "    print(f\"\\nPrediction: '{pred}' â†’ '{pred_norm}'\")\n",
    "    print(f\"Truth:      '{truth}' â†’ '{truth_norm}'\")\n",
    "    print(f\"EM Score:   {em:.0%} {'âœ“' if em == 1.0 else 'âœ—'}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"   Normalization fixes 'The answer' vs 'answer' â†’ EM = 100%\")\n",
    "print(\"   But 'New York City' vs 'New York' â†’ Still EM = 0% (too strict!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Overall Exact Match: 34.00%\n",
      "============================================================\n",
      "\n",
      "This means 34.0% of BERT's predictions are EXACTLY correct.\n",
      "But what about the other 66.0%? Are they all wrong?\n"
     ]
    }
   ],
   "source": [
    "# Compute EM for all predictions\n",
    "em_score = compute_em_for_dataset(predictions)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overall Exact Match: {em_score:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nThis means {em_score:.1f}% of BERT's predictions are EXACTLY correct.\")\n",
    "print(f\"But what about the other {100-em_score:.1f}%? Are they all wrong?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: F1 Score - The Partial Credit Metric (1.5 minutes)\n",
    "\n",
    "**Problem with EM:** Too strict! \"New York City\" vs \"New York\" gets 0% even though it's mostly correct.\n",
    "\n",
    "**Solution: F1 Score**\n",
    "- Measures token-level overlap\n",
    "- Balances **precision** (no extra words) and **recall** (got the words)\n",
    "- Gives partial credit!\n",
    "\n",
    "**Formula:**\n",
    "- Precision = |predicted âˆ© reference| / |predicted|\n",
    "- Recall = |predicted âˆ© reference| / |reference|\n",
    "- F1 = 2 Ã— (precision Ã— recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score Calculation Example:\n",
      "============================================================\n",
      "Prediction: 'New York City'\n",
      "Truth:      'New York'\n",
      "\n",
      "Predicted tokens: ['new', 'york', 'city']\n",
      "Truth tokens:     ['new', 'york']\n",
      "\n",
      "Overlap: {'new', 'york'}\n",
      "\n",
      "Precision: 2/3 = 66.67%\n",
      "Recall:    2/2 = 100.00%\n",
      "F1 Score:  80.00%\n",
      "\n",
      "Compare to EM: 0%\n",
      "\n",
      "ðŸ’¡ Key Insight:\n",
      "   EM gives 0% (too harsh!)\n",
      "   F1 gives 67% (partial credit for 2 out of 3 words!)\n"
     ]
    }
   ],
   "source": [
    "# Show F1 calculation step-by-step\n",
    "pred = \"New York City\"\n",
    "truth = \"New York\"\n",
    "\n",
    "print(\"F1 Score Calculation Example:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prediction: '{pred}'\")\n",
    "print(f\"Truth:      '{truth}'\")\n",
    "print()\n",
    "\n",
    "from metrics import get_tokens\n",
    "pred_tokens = get_tokens(pred)\n",
    "truth_tokens = get_tokens(truth)\n",
    "\n",
    "print(f\"Predicted tokens: {pred_tokens}\")\n",
    "print(f\"Truth tokens:     {truth_tokens}\")\n",
    "print()\n",
    "\n",
    "overlap = set(pred_tokens) & set(truth_tokens)\n",
    "print(f\"Overlap: {overlap}\")\n",
    "print()\n",
    "\n",
    "precision = len(overlap) / len(pred_tokens)\n",
    "recall = len(overlap) / len(truth_tokens)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Precision: {len(overlap)}/{len(pred_tokens)} = {precision:.2%}\")\n",
    "print(f\"Recall:    {len(overlap)}/{len(truth_tokens)} = {recall:.2%}\")\n",
    "print(f\"F1 Score:  {f1:.2%}\")\n",
    "print()\n",
    "\n",
    "em = compute_exact_match(pred, truth)\n",
    "print(f\"Compare to EM: {em:.0%}\")\n",
    "print()\n",
    "print(\"ðŸ’¡ Key Insight:\")\n",
    "print(\"   EM gives 0% (too harsh!)\")\n",
    "print(\"   F1 gives 67% (partial credit for 2 out of 3 words!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Overall F1 Score: 42.33%\n",
      "============================================================\n",
      "\n",
      "Compare:\n",
      "  Exact Match (EM): 34.00%\n",
      "  F1 Score:         42.33%\n",
      "  Difference:       +8.33 percentage points\n",
      "\n",
      "ðŸ’¡ F1 is higher because it gives partial credit!\n",
      "   This better reflects BERT's actual quality.\n"
     ]
    }
   ],
   "source": [
    "# Compute F1 for all predictions\n",
    "f1_score = compute_f1_for_dataset(predictions)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overall F1 Score: {f1_score:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nCompare:\")\n",
    "print(f\"  Exact Match (EM): {em_score:.2f}%\")\n",
    "print(f\"  F1 Score:         {f1_score:.2f}%\")\n",
    "print(f\"  Difference:       +{f1_score - em_score:.2f} percentage points\")\n",
    "print()\n",
    "print(\"ðŸ’¡ F1 is higher because it gives partial credit!\")\n",
    "print(\"   This better reflects BERT's actual quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Answerable vs Unanswerable (1 minute)\n",
    "\n",
    "**SQuAD 2.0 Twist:** Some questions are **unanswerable**!\n",
    "\n",
    "Model should predict empty string `\"\"` for these.\n",
    "\n",
    "Let's see how BERT handles both types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRICS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Overall (100 examples):\n",
      "  Exact Match (EM):  34.00%\n",
      "  F1 Score:          42.33%\n",
      "\n",
      "Answerable Questions (53 examples):\n",
      "  Exact Match (EM):  64.15%\n",
      "  F1 Score:          79.87%\n",
      "\n",
      "Unanswerable Questions (47 examples):\n",
      "  Exact Match (EM):  0.00%\n",
      "  F1 Score:          0.00%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics by question type\n",
    "metrics_by_type = compute_metrics_by_type(predictions)\n",
    "\n",
    "# Pretty print\n",
    "print_metrics_summary(metrics_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¡ Key Observation:\n",
      "   BERT performs MUCH better on answerable questions!\n",
      "   Unanswerable detection is harder - model struggles to say 'I don't know'.\n"
     ]
    }
   ],
   "source": [
    "# Visualize the breakdown\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = ['Overall', 'Answerable', 'Unanswerable']\n",
    "em_scores = [\n",
    "    metrics_by_type['overall']['exact_match'],\n",
    "    metrics_by_type['answerable']['exact_match'] if metrics_by_type['answerable'] else 0,\n",
    "    metrics_by_type['unanswerable']['exact_match'] if metrics_by_type['unanswerable'] else 0\n",
    "]\n",
    "f1_scores = [\n",
    "    metrics_by_type['overall']['f1'],\n",
    "    metrics_by_type['answerable']['f1'] if metrics_by_type['answerable'] else 0,\n",
    "    metrics_by_type['unanswerable']['f1'] if metrics_by_type['unanswerable'] else 0\n",
    "]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, em_scores, width, label='Exact Match', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='coral')\n",
    "\n",
    "ax.set_ylabel('Score (%)', fontsize=12)\n",
    "ax.set_title('BERT Q&A Performance: Answerable vs Unanswerable Questions', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Observation:\")\n",
    "print(\"   BERT performs MUCH better on answerable questions!\")\n",
    "print(\"   Unanswerable detection is harder - model struggles to say 'I don't know'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Validate with HuggingFace Evaluate (1 minute)\n",
    "\n",
    "**Critical Step:** Always validate custom metrics against official implementations!\n",
    "\n",
    "HuggingFace provides the **official SQuAD metric**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded official SQuAD v2 metric from HuggingFace Evaluate\n"
     ]
    }
   ],
   "source": [
    "# Install evaluate library if needed\n",
    "import evaluate\n",
    "\n",
    "# Load official SQuAD v2 metric\n",
    "squad_metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "print(\"âœ“ Loaded official SQuAD v2 metric from HuggingFace Evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VALIDATION: Our Implementation vs Official SQuAD Metric\n",
      "======================================================================\n",
      "\n",
      "Our Exact Match:      34.00%\n",
      "Official Exact Match: 34.00%\n",
      "Difference:           0.00 points\n",
      "\n",
      "Our F1 Score:         42.33%\n",
      "Official F1 Score:    42.33%\n",
      "Difference:           0.00 points\n",
      "\n",
      "âœ“ Our implementation matches the official metric! âœ“\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Format predictions for official metric\n",
    "formatted_predictions = [\n",
    "    {'id': p['id'], 'prediction_text': p['prediction_text'], 'no_answer_probability': 0.0}\n",
    "    for p in predictions\n",
    "]\n",
    "\n",
    "formatted_references = [\n",
    "    {'id': p['id'], 'answers': {'text': [p['ground_truth']] if p['ground_truth'] else [], 'answer_start': [0]}}\n",
    "    for p in predictions\n",
    "]\n",
    "\n",
    "# Compute official metrics\n",
    "official_results = squad_metric.compute(\n",
    "    predictions=formatted_predictions,\n",
    "    references=formatted_references\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION: Our Implementation vs Official SQuAD Metric\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOur Exact Match:      {em_score:.2f}%\")\n",
    "print(f\"Official Exact Match: {official_results['exact']:.2f}%\")\n",
    "print(f\"Difference:           {abs(em_score - official_results['exact']):.2f} points\")\n",
    "print()\n",
    "print(f\"Our F1 Score:         {f1_score:.2f}%\")\n",
    "print(f\"Official F1 Score:    {official_results['f1']:.2f}%\")\n",
    "print(f\"Difference:           {abs(f1_score - official_results['f1']):.2f} points\")\n",
    "print()\n",
    "if abs(em_score - official_results['exact']) < 1.0 and abs(f1_score - official_results['f1']) < 1.0:\n",
    "    print(\"âœ“ Our implementation matches the official metric! âœ“\")\n",
    "else:\n",
    "    print(\"âš  Small differences may exist due to rounding or edge cases.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Quick Overview of Ranking Metrics (0.5 minutes)\n",
    "\n",
    "**For ranked retrieval systems** (when model returns top-K candidates):\n",
    "\n",
    "- **Precision@K:** What fraction of top-K are correct? (Quality)\n",
    "- **Recall@K:** What fraction of all correct answers are in top-K? (Coverage)\n",
    "- **MRR:** How highly is the first correct answer ranked? (User experience)\n",
    "\n",
    "**Example:**\n",
    "- Model returns: [\"Paris\", \"France\", \"Paris, France\"]\n",
    "- Ground truth: [\"Paris\"]\n",
    "- P@3 = 1/3 = 33% (1 correct out of 3)\n",
    "- R@3 = 1/1 = 100% (got the 1 correct answer)\n",
    "- MRR = 1/1 = 1.0 (correct answer at rank 1)\n",
    "\n",
    "**Note:** We focus on EM/F1 in this demo. Ranking metrics are important for search/retrieval systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Which Metrics to Use?\n",
    "\n",
    "## Metric Selection Guide\n",
    "\n",
    "| Task | Recommended Metrics | Why? |\n",
    "|------|-------------------|------|\n",
    "| **Single-answer Q&A** | EM + F1 | EM for strict correctness, F1 for partial credit |\n",
    "| **Ranked retrieval** | P@K + R@K + MRR | Quality (P@K), Coverage (R@K), Ranking (MRR) |\n",
    "| **Unanswerable detection** | EM + F1 separately | Track performance on both types |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. âœ… **EM is strict but interpretable** - Binary right/wrong\n",
    "2. âœ… **F1 gives partial credit** - Better correlates with human judgment\n",
    "3. âœ… **Normalization is critical** - \"The answer\" vs \"answer\" should match\n",
    "4. âœ… **Always use multiple metrics** - Each captures different aspects\n",
    "5. âœ… **Validate with official implementations** - HuggingFace Evaluate\n",
    "6. âœ… **Break down by question type** - Answerable vs unanswerable\n",
    "\n",
    "## Connection to Lesson 6\n",
    "\n",
    "**Lesson 6:** Explored BERT's attention mechanisms\n",
    "- Visualized how BERT \"looks\" at different words\n",
    "- Understood contextualized embeddings\n",
    "\n",
    "**Lesson 7:** Applied BERT to question answering\n",
    "- Used same BERT architecture for Q&A\n",
    "- Measured if BERT's answers are correct\n",
    "- Found: F1 ~78% - BERT is pretty good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
