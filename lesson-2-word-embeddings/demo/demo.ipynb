{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bda0bef",
   "metadata": {},
   "source": [
    "# Demo: Exploring Pretrained Word Embeddings\n",
    "\n",
    "**Estimated Time**: 6 minutes | **Skill Pair 2**: Word Embeddings\n",
    "\n",
    "**Scenario**: Your news recommendation system currently can't tell that \"climate\" and \"environment\" articles are related. By using word embeddings that capture semantic similarity, you can recommend articles about \"renewable energy\" to readers interested in \"solar power\"‚Äîeven if they don't share exact words.\n",
    "\n",
    "**What You'll Discover**: Why one-hot encoding fails, how embeddings capture meaning in geometry, and why \"king - man + woman ‚âà queen\" actually works!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe4f23",
   "metadata": {},
   "source": [
    "## ü§ñ Why Word Embeddings Matter\n",
    "\n",
    "**The Problem**: One-hot encoding treats every word as equally different:\n",
    "- **\"dog\"** and **\"cat\"** ‚Üí completely unrelated (distance = ‚àö2)\n",
    "- **\"dog\"** and **\"mathematics\"** ‚Üí also completely unrelated (distance = ‚àö2)\n",
    "\n",
    "**But we know**: \"dog\" is more similar to \"cat\" than to \"mathematics\"!\n",
    "\n",
    "**The Solution**: Word embeddings place semantically similar words close together in vector space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf268aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "PyTorch version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader as api\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88acdd",
   "metadata": {},
   "source": [
    "## üö® The One-Hot Problem\n",
    "\n",
    "Let's see why one-hot encoding fails for capturing word meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2325bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ One-Hot Encoding Example:\n",
      "dog          ‚Üí [1. 0. 0. 0. 0. 0.]\n",
      "cat          ‚Üí [0. 1. 0. 0. 0. 0.]\n",
      "animal       ‚Üí [0. 0. 1. 0. 0. 0.]\n",
      "mathematics  ‚Üí [0. 0. 0. 1. 0. 0.]\n",
      "equation     ‚Üí [0. 0. 0. 0. 1. 0.]\n",
      "number       ‚Üí [0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "üìè Distances between words:\n",
      "   dog ‚Üî cat:         1.414\n",
      "   dog ‚Üî mathematics: 1.414\n",
      "\n",
      "‚ùå Problem: All words are equally distant!\n",
      "   - Can't capture that 'dog' and 'cat' are both animals\n",
      "   - 50,000 word vocabulary = 50,000 dimensions (sparse!)\n",
      "   - No notion of semantic similarity\n"
     ]
    }
   ],
   "source": [
    "def create_one_hot_demo():\n",
    "    \"\"\"Demonstrate the limitations of one-hot encoding.\"\"\"\n",
    "    \n",
    "    # Simple vocabulary for demonstration\n",
    "    vocab = ['dog', 'cat', 'animal', 'mathematics', 'equation', 'number']\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    # Create one-hot vectors\n",
    "    one_hot_vectors = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "        vector = np.zeros(vocab_size)\n",
    "        vector[i] = 1\n",
    "        one_hot_vectors[word] = vector\n",
    "    \n",
    "    print(\"üî¢ One-Hot Encoding Example:\")\n",
    "    for word, vector in one_hot_vectors.items():\n",
    "        print(f\"{word:12} ‚Üí {vector}\")\n",
    "    \n",
    "    # Calculate distances between words\n",
    "    def euclidean_distance(v1, v2):\n",
    "        return np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "    \n",
    "    print(\"\\nüìè Distances between words:\")\n",
    "    dog_vec = one_hot_vectors['dog']\n",
    "    cat_vec = one_hot_vectors['cat']\n",
    "    math_vec = one_hot_vectors['mathematics']\n",
    "    \n",
    "    dist_dog_cat = euclidean_distance(dog_vec, cat_vec)\n",
    "    dist_dog_math = euclidean_distance(dog_vec, math_vec)\n",
    "    \n",
    "    print(f\"   dog ‚Üî cat:         {dist_dog_cat:.3f}\")\n",
    "    print(f\"   dog ‚Üî mathematics: {dist_dog_math:.3f}\")\n",
    "    \n",
    "    print(\"\\n‚ùå Problem: All words are equally distant!\")\n",
    "    print(\"   - Can't capture that 'dog' and 'cat' are both animals\")\n",
    "    print(\"   - 50,000 word vocabulary = 50,000 dimensions (sparse!)\")\n",
    "    print(\"   - No notion of semantic similarity\")\n",
    "\n",
    "create_one_hot_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07ce6d",
   "metadata": {},
   "source": [
    "## üß† Two Approaches to Word Embeddings\n",
    "\n",
    "**Word2Vec**: Predicts context from word (or word from context)\n",
    "- **Skip-gram**: Given \"cat\", predict [\"the\", \"sat\", \"on\", \"mat\"]\n",
    "- **CBOW**: Given [\"the\", \"sat\", \"on\", \"mat\"], predict \"cat\"\n",
    "\n",
    "**GloVe**: Global co-occurrence statistics\n",
    "- Count how often words appear together across entire corpus\n",
    "- Factorize co-occurrence matrix into dense vectors\n",
    "- Captures global statistical information\n",
    "\n",
    "**Both produce**: Dense 300-dimensional vectors instead of 50,000-dimensional one-hot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e43bd4",
   "metadata": {},
   "source": [
    "## üì• Load Pretrained GloVe Embeddings\n",
    "\n",
    "We'll use GloVe vectors trained on 6 billion tokens from Wikipedia and Gigaword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c8b4dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading GloVe embeddings (this may take a moment)...\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "‚úÖ Loaded GloVe embeddings!\n",
      "   Vocabulary size: 400,000 words\n",
      "   Vector dimension: 100\n",
      "   Training corpus: Wikipedia + Gigaword (6B tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained GloVe embeddings\n",
    "print(\"üì• Loading GloVe embeddings (this may take a moment)...\")\n",
    "\n",
    "# Load smaller GloVe model for demo (100d instead of 300d for speed)\n",
    "try:\n",
    "    glove_model = api.load('glove-wiki-gigaword-100')\n",
    "    print(f\"‚úÖ Loaded GloVe embeddings!\")\n",
    "    print(f\"   Vocabulary size: {len(glove_model):,} words\")\n",
    "    print(f\"   Vector dimension: {glove_model.vector_size}\")\n",
    "    print(f\"   Training corpus: Wikipedia + Gigaword (6B tokens)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading GloVe: {e}\")\n",
    "    print(\"   Note: This requires internet connection for first download\")\n",
    "    glove_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff93f2",
   "metadata": {},
   "source": [
    "## üîç Explore Word Vectors\n",
    "\n",
    "Let's look up embeddings for news-related words and see how they relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307df0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ News Category Word Vectors:\n",
      "climate      ‚Üí [-1.0901    -0.0036324  1.4329     0.45647   -0.01104  ]... (showing first 5 of 100 dims)\n",
      "environment  ‚Üí [-0.74272   0.1349    0.68435  -0.077705  0.026786]... (showing first 5 of 100 dims)\n",
      "weather      ‚Üí [-1.077    -0.42305   0.72816   0.031298 -0.85608 ]... (showing first 5 of 100 dims)\n",
      "politics     ‚Üí [-0.54286   0.45469   0.64719  -0.22052   0.091599]... (showing first 5 of 100 dims)\n",
      "sports       ‚Üí [ 0.25178  0.21679 -0.18549 -0.60748 -0.5374 ]... (showing first 5 of 100 dims)\n",
      "technology   ‚Üí [-0.12241   0.64795   0.43668   0.011368  0.50016 ]... (showing first 5 of 100 dims)\n",
      "\n",
      "‚ú® Key insight: Each word is now a dense 100-dimensional vector!\n",
      "   Unlike one-hot, these vectors can capture semantic relationships.\n"
     ]
    }
   ],
   "source": [
    "if glove_model is not None:\n",
    "    # News-related words for our recommendation scenario\n",
    "    news_words = ['climate', 'environment', 'weather', 'politics', 'sports', 'technology']\n",
    "    \n",
    "    print(\"üì∞ News Category Word Vectors:\")\n",
    "    word_vectors = {}\n",
    "    \n",
    "    for word in news_words:\n",
    "        if word in glove_model:\n",
    "            vector = glove_model[word]\n",
    "            word_vectors[word] = vector\n",
    "            print(f\"{word:12} ‚Üí {vector[:5]}... (showing first 5 of {len(vector)} dims)\")\n",
    "        else:\n",
    "            print(f\"{word:12} ‚Üí Not found in vocabulary\")\n",
    "    \n",
    "    print(f\"\\n‚ú® Key insight: Each word is now a dense {glove_model.vector_size}-dimensional vector!\")\n",
    "    print(\"   Unlike one-hot, these vectors can capture semantic relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2baf4b0",
   "metadata": {},
   "source": [
    "## üìê Measuring Semantic Similarity\n",
    "\n",
    "Cosine similarity measures the angle between vectors‚Äîperfect for capturing semantic relatedness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746ac6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Cosine Similarity Matrix (1.0 = identical, 0.0 = unrelated):\n",
      "            climate environmentweather politicssports  technology\n",
      "climate        1.000   0.760   0.632   0.401   0.195   0.414\n",
      "environment    0.760   1.000   0.522   0.454   0.343   0.556\n",
      "weather        0.632   0.522   1.000   0.246   0.330   0.357\n",
      "politics       0.401   0.454   0.246   1.000   0.452   0.413\n",
      "sports         0.195   0.343   0.330   0.452   1.000   0.419\n",
      "technology     0.414   0.556   0.357   0.413   0.419   1.000\n",
      "\n",
      "üéØ Key Insights:\n",
      "   üìà 'climate' ‚Üî 'environment': 0.760 (high similarity!)\n",
      "   üìä 'climate' ‚Üî 'sports': 0.195 (low similarity)\n",
      "\n",
      "üí° This is how recommendation systems work!\n",
      "   Users reading 'climate' articles ‚Üí recommend 'environment' content\n"
     ]
    }
   ],
   "source": [
    "if glove_model is not None and word_vectors:\n",
    "    def calculate_similarity_matrix(words, model):\n",
    "        \"\"\"Calculate cosine similarity between all pairs of words.\"\"\"\n",
    "        available_words = [w for w in words if w in model]\n",
    "        vectors = [model[w] for w in available_words]\n",
    "        \n",
    "        # Calculate cosine similarity matrix\n",
    "        similarity_matrix = cosine_similarity(vectors)\n",
    "        \n",
    "        return available_words, similarity_matrix\n",
    "    \n",
    "    # Calculate similarities\n",
    "    words, sim_matrix = calculate_similarity_matrix(news_words, glove_model)\n",
    "    \n",
    "    print(\"üîó Cosine Similarity Matrix (1.0 = identical, 0.0 = unrelated):\")\n",
    "    print(f\"{'':12}\", end=\"\")\n",
    "    for word in words:\n",
    "        print(f\"{word:8}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, word1 in enumerate(words):\n",
    "        print(f\"{word1:12}\", end=\"\")\n",
    "        for j, word2 in enumerate(words):\n",
    "            similarity = sim_matrix[i][j]\n",
    "            print(f\"{similarity:8.3f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Highlight key relationships\n",
    "    print(\"\\nüéØ Key Insights:\")\n",
    "    if 'climate' in words and 'environment' in words:\n",
    "        climate_env_sim = glove_model.similarity('climate', 'environment')\n",
    "        print(f\"   üìà 'climate' ‚Üî 'environment': {climate_env_sim:.3f} (high similarity!)\")\n",
    "    \n",
    "    if 'climate' in words and 'sports' in words:\n",
    "        climate_sports_sim = glove_model.similarity('climate', 'sports')\n",
    "        print(f\"   üìä 'climate' ‚Üî 'sports': {climate_sports_sim:.3f} (low similarity)\")\n",
    "    \n",
    "    print(\"\\nüí° This is how recommendation systems work!\")\n",
    "    print(\"   Users reading 'climate' articles ‚Üí recommend 'environment' content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a00769",
   "metadata": {},
   "source": [
    "## ü™Ñ Word Arithmetic: The Magic of Embeddings\n",
    "\n",
    "The famous example: **king - man + woman ‚âà queen**\n",
    "\n",
    "This works because embeddings capture semantic relationships as geometric patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d1a15bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü™Ñ Word Arithmetic Examples:\n",
      "\n",
      "1Ô∏è‚É£ king - man + woman =\n",
      "   queen           (similarity: 0.770)\n",
      "   monarch         (similarity: 0.684)\n",
      "   throne          (similarity: 0.676)\n",
      "   daughter        (similarity: 0.659)\n",
      "   princess        (similarity: 0.652)\n",
      "   ‚ú® Top result: 'queen' - The arithmetic worked!\n",
      "\n",
      "2Ô∏è‚É£ paris - france + italy =\n",
      "   rome            (similarity: 0.819)\n",
      "   milan           (similarity: 0.738)\n",
      "   naples          (similarity: 0.712)\n",
      "   üó∫Ô∏è Top result: 'rome' (capital of Italy!)\n",
      "\n",
      "3Ô∏è‚É£ News domain: climate - environment + technology =\n",
      "   technologies    (similarity: 0.649)\n",
      "   global          (similarity: 0.613)\n",
      "   tech            (similarity: 0.608)\n",
      "   üî¨ Interesting blend of climate and tech concepts!\n",
      "\n",
      "üéØ Why this works:\n",
      "   - Embeddings learn that 'gender' is a consistent direction\n",
      "   - 'Capital city' relationships are captured geometrically\n",
      "   - Vector arithmetic preserves these semantic relationships\n"
     ]
    }
   ],
   "source": [
    "if glove_model is not None:\n",
    "    def word_arithmetic(model, positive_words, negative_words, top_n=5):\n",
    "        \"\"\"Perform word arithmetic: positive_words - negative_words.\"\"\"\n",
    "        try:\n",
    "            # Use gensim's built-in most_similar method\n",
    "            results = model.most_similar(\n",
    "                positive=positive_words, \n",
    "                negative=negative_words, \n",
    "                topn=top_n\n",
    "            )\n",
    "            return results\n",
    "        except KeyError as e:\n",
    "            return f\"Word not found: {e}\"\n",
    "    \n",
    "    print(\"ü™Ñ Word Arithmetic Examples:\\n\")\n",
    "    \n",
    "    # Classic example: king - man + woman\n",
    "    print(\"1Ô∏è‚É£ king - man + woman =\")\n",
    "    results = word_arithmetic(glove_model, ['king', 'woman'], ['man'])\n",
    "    if isinstance(results, list):\n",
    "        for word, similarity in results:\n",
    "            print(f\"   {word:15} (similarity: {similarity:.3f})\")\n",
    "        print(f\"   ‚ú® Top result: '{results[0][0]}' - The arithmetic worked!\")\n",
    "    else:\n",
    "        print(f\"   {results}\")\n",
    "    \n",
    "    print(\"\\n2Ô∏è‚É£ paris - france + italy =\")\n",
    "    results = word_arithmetic(glove_model, ['paris', 'italy'], ['france'])\n",
    "    if isinstance(results, list):\n",
    "        for word, similarity in results[:3]:\n",
    "            print(f\"   {word:15} (similarity: {similarity:.3f})\")\n",
    "        print(f\"   üó∫Ô∏è Top result: '{results[0][0]}' (capital of Italy!)\")\n",
    "    \n",
    "    print(\"\\n3Ô∏è‚É£ News domain: climate - environment + technology =\")\n",
    "    results = word_arithmetic(glove_model, ['climate', 'technology'], ['environment'])\n",
    "    if isinstance(results, list):\n",
    "        for word, similarity in results[:3]:\n",
    "            print(f\"   {word:15} (similarity: {similarity:.3f})\")\n",
    "        print(f\"   üî¨ Interesting blend of climate and tech concepts!\")\n",
    "    \n",
    "    print(\"\\nüéØ Why this works:\")\n",
    "    print(\"   - Embeddings learn that 'gender' is a consistent direction\")\n",
    "    print(\"   - 'Capital city' relationships are captured geometrically\")\n",
    "    print(\"   - Vector arithmetic preserves these semantic relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9f964",
   "metadata": {},
   "source": [
    "## üé≤ Compare to Random Embeddings\n",
    "\n",
    "Let's prove that these relationships aren't just coincidence by comparing to random vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1edcf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Random vs. Pretrained Embedding Comparison:\n",
      "\n",
      "üëë King-Queen Similarity:\n",
      "   GloVe:  0.751 (high - they're related!)\n",
      "   Random: -0.138 (low - no relationship captured)\n",
      "\n",
      "üåç Climate-Environment Similarity:\n",
      "   GloVe:  0.760 (high - semantically related)\n",
      "   Random: -0.082 (random - no meaning)\n",
      "\n",
      "‚úÖ Conclusion: Pretrained embeddings capture real semantic relationships!\n",
      "   Random vectors can't distinguish related from unrelated concepts.\n"
     ]
    }
   ],
   "source": [
    "if glove_model is not None:\n",
    "    # Create random embeddings for comparison\n",
    "    np.random.seed(42)\n",
    "    random_embeddings = {}\n",
    "    test_words = ['king', 'queen', 'man', 'woman', 'climate', 'environment']\n",
    "    \n",
    "    print(\"üé≤ Random vs. Pretrained Embedding Comparison:\\n\")\n",
    "    \n",
    "    for word in test_words:\n",
    "        if word in glove_model:\n",
    "            # Random 100-dimensional vector\n",
    "            random_embeddings[word] = np.random.normal(0, 1, 100)\n",
    "    \n",
    "    # Compare similarities\n",
    "    def cosine_sim(v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    print(\"üëë King-Queen Similarity:\")\n",
    "    if 'king' in glove_model and 'queen' in glove_model:\n",
    "        glove_sim = glove_model.similarity('king', 'queen')\n",
    "        random_sim = cosine_sim(random_embeddings['king'], random_embeddings['queen'])\n",
    "        print(f\"   GloVe:  {glove_sim:.3f} (high - they're related!)\")\n",
    "        print(f\"   Random: {random_sim:.3f} (low - no relationship captured)\")\n",
    "    \n",
    "    print(\"\\nüåç Climate-Environment Similarity:\")\n",
    "    if 'climate' in glove_model and 'environment' in glove_model:\n",
    "        glove_sim = glove_model.similarity('climate', 'environment')\n",
    "        random_sim = cosine_sim(random_embeddings['climate'], random_embeddings['environment'])\n",
    "        print(f\"   GloVe:  {glove_sim:.3f} (high - semantically related)\")\n",
    "        print(f\"   Random: {random_sim:.3f} (random - no meaning)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Conclusion: Pretrained embeddings capture real semantic relationships!\")\n",
    "    print(\"   Random vectors can't distinguish related from unrelated concepts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
