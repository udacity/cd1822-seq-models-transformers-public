{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d9c112",
   "metadata": {},
   "source": [
    "# Exercise: Analyze Word Relationships with Embedding Arithmetic\n",
    "\n",
    "**Estimated Time**: 15 minutes | **Status**: üöß Your implementation needed\n",
    "\n",
    "**Scenario**: Before committing to GloVe embeddings for your news recommender, the data science team wants evidence they actually capture the relationships you care about: category similarities (politics/government), writing style (formal/casual), and topic clustering (technology subcategories). Test whether pretrained embeddings understand news domain semantics.\n",
    "\n",
    "**What You'll Learn**: Embedding relationships, analogy evaluation, visualization techniques, and coverage analysis for domain-specific applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41d127",
   "metadata": {},
   "source": [
    "## üéØ Why This Matters for Production Systems\n",
    "\n",
    "**Real-world question**: Should we use pretrained GloVe or train custom embeddings?\n",
    "\n",
    "**This analysis helps decide**:\n",
    "- Do pretrained embeddings capture **news domain semantics**?\n",
    "- What's the **vocabulary coverage** on our specific dataset?\n",
    "- Where do we need **domain-specific training**?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader as api\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987e317",
   "metadata": {},
   "source": [
    "## üì• Load Data and Embeddings\n",
    "\n",
    "First, let's load both our AG News dataset and pretrained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eef4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News dataset\n",
    "print(\"üì∞ Loading AG News dataset...\")\n",
    "try:\n",
    "    # Load full dataset for comprehensive analysis\n",
    "    dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "    \n",
    "    # Category mapping\n",
    "    categories = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Technology\"}\n",
    "    category_names = list(categories.values())\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(dataset):,} news articles\")\n",
    "    print(f\"üìä Categories: {category_names}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    label_counts = Counter([dataset[i]['label'] for i in range(len(dataset))])\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"   {categories[label]}: {count:,} articles\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "print(\"üì• Loading GloVe embeddings...\")\n",
    "try:\n",
    "    # Use 100d model for faster processing\n",
    "    glove_model = api.load('glove-wiki-gigaword-100')\n",
    "    print(f\"‚úÖ Loaded GloVe embeddings!\")\n",
    "    print(f\"   Vocabulary size: {len(glove_model):,} words\")\n",
    "    print(f\"   Vector dimension: {glove_model.vector_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading GloVe: {e}\")\n",
    "    print(\"   Note: This requires internet connection for first download\")\n",
    "    glove_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc98e3",
   "metadata": {},
   "source": [
    "## Part A: Find Similar Words\n",
    "\n",
    "Build a similarity function and test it on news-relevant words.\n",
    "\n",
    "**Your task**: Complete the `find_similar()` function to find words with highest cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b39b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if glove_model is not None:\n",
    "    def find_similar(word, n=10, model=glove_model):\n",
    "        \"\"\"\n",
    "        Find n most similar words using cosine similarity.\n",
    "        \n",
    "        Args:\n",
    "            word: Target word to find similarities for\n",
    "            n: Number of similar words to return\n",
    "            model: Embedding model to use\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # TODO: Use gensim's built-in similarity function to find most similar words\n",
    "            similar_words = None  # Replace this line\n",
    "            return similar_words\n",
    "        except KeyError:\n",
    "            return f\"'{word}' not found in vocabulary\"\n",
    "    \n",
    "    # Test on news-relevant words\n",
    "    test_words = [\"election\", \"technology\", \"economy\"]\n",
    "    \n",
    "    print(\"üîç Finding similar words for news-relevant terms:\\n\")\n",
    "    \n",
    "    for word in test_words:\n",
    "        print(f\"üìç Words similar to '{word}':\")\n",
    "        similar = find_similar(word, n=8)\n",
    "        \n",
    "        if isinstance(similar, list):\n",
    "            for similar_word, score in similar:\n",
    "                print(f\"   {similar_word:<15} (similarity: {score:.3f})\")\n",
    "        else:\n",
    "            print(f\"   {similar}\")\n",
    "        print()\n",
    "    \n",
    "    # TODO: Analyze the patterns you observe\n",
    "    print(\"üéØ YOUR OBSERVATIONS:\")\n",
    "    print(\"   What patterns do you notice in the similar words?\")\n",
    "    print(\"   Are the relationships meaningful for news categorization?\")\n",
    "    print(\"   YOUR ANALYSIS HERE...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346e5ec",
   "metadata": {},
   "source": [
    "## Part B: Explore Semantic Analogies\n",
    "\n",
    "Test whether embeddings capture logical relationships through vector arithmetic.\n",
    "\n",
    "**Your task**: Complete the analogy function and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if glove_model is not None:\n",
    "    def test_analogy(word1, word2, word3, expected=None, model=glove_model, top_n=5):\n",
    "        \"\"\"\n",
    "        Test analogy: word1 is to word2 as word3 is to ?\n",
    "        \n",
    "        Args:\n",
    "            word1, word2, word3: The analogy components  \n",
    "            expected: Expected result (for evaluation)\n",
    "            model: Embedding model\n",
    "            top_n: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, similarity) results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # TODO: Implement vector arithmetic for analogies\n",
    "            # Vector arithmetic: word2 - word1 + word3\n",
    "            results = None  # Replace this line\n",
    "            return results\n",
    "        except KeyError as e:\n",
    "            return f\"Word not found: {e}\"\n",
    "    \n",
    "    print(\"ü™Ñ Testing Semantic Analogies:\\n\")\n",
    "    \n",
    "    # Test cases with expected answers\n",
    "    analogies = [\n",
    "        (\"good\", \"better\", \"bad\", \"worse\"),\n",
    "        (\"president\", \"politics\", \"quarterback\", \"sports\"),\n",
    "        (\"newspaper\", \"journalism\", \"television\", \"broadcasting\"),\n",
    "        (\"stock\", \"business\", \"election\", \"politics\"),\n",
    "        (\"computer\", \"technology\", \"athlete\", \"sports\"),\n",
    "        (\"profit\", \"business\", \"victory\", \"sports\"),\n",
    "        (\"software\", \"technology\", \"legislation\", \"politics\")\n",
    "    ]\n",
    "    \n",
    "    successful_analogies = 0\n",
    "    total_analogies = len(analogies)\n",
    "    \n",
    "    for i, (w1, w2, w3, expected) in enumerate(analogies, 1):\n",
    "        print(f\"{i}Ô∏è‚É£ {w1} : {w2} :: {w3} : ?  (expecting '{expected}')\")\n",
    "        \n",
    "        results = test_analogy(w1, w2, w3, expected)\n",
    "        \n",
    "        if isinstance(results, list) and results:\n",
    "            # Show top 3 results\n",
    "            for j, (word, similarity) in enumerate(results[:3]):\n",
    "                marker = \"‚úÖ\" if word.lower() == expected.lower() else \"  \"\n",
    "                print(f\"   {marker} {word:<15} (similarity: {similarity:.3f})\")\n",
    "            \n",
    "            # Check if expected word is in top 3\n",
    "            top_3_words = [word.lower() for word, _ in results[:3]]\n",
    "            if expected.lower() in top_3_words:\n",
    "                successful_analogies += 1\n",
    "                print(f\"   üéØ SUCCESS: Found '{expected}' in top 3!\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Expected '{expected}' not in top 3\")\n",
    "        else:\n",
    "            print(f\"   {results}\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate success rate\n",
    "    success_rate = (successful_analogies / total_analogies) * 100\n",
    "    print(f\"üìä Analogy Success Rate: {successful_analogies}/{total_analogies} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # TODO: Interpret the results\n",
    "    print(\"\\nü§î YOUR INTERPRETATION:\")\n",
    "    print(\"   What does this performance suggest about using GloVe for news?\")\n",
    "    print(\"   Do the embeddings capture meaningful semantic relationships?\")\n",
    "    print(\"   YOUR ANALYSIS HERE...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab12586",
   "metadata": {},
   "source": [
    "## Part C: Visualize Word Clusters\n",
    "\n",
    "Extract representative words from each news category and visualize their embedding space.\n",
    "\n",
    "**Your task**: Complete the word embedding extraction for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset is not None:\n",
    "    def extract_category_words(dataset, categories, words_per_category=10):\n",
    "        \"\"\"\n",
    "        Extract most frequent words from each category.\n",
    "        \"\"\"\n",
    "        category_words = {}\n",
    "        \n",
    "        for label, category_name in categories.items():\n",
    "            # Get all texts for this category\n",
    "            category_texts = []\n",
    "            for article in dataset:\n",
    "                if article['label'] == label:\n",
    "                    category_texts.append(article['text'].lower())\n",
    "            \n",
    "            # Extract and count words\n",
    "            all_words = []\n",
    "            for text in category_texts:\n",
    "                # Simple tokenization\n",
    "                words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "                all_words.extend(words)\n",
    "            \n",
    "            # Get most frequent words (excluding common stop words)\n",
    "            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'shall', 'said', 'says', 'say'}\n",
    "            \n",
    "            word_counts = Counter([w for w in all_words if len(w) > 3 and w not in stop_words])\n",
    "            most_frequent = [word for word, count in word_counts.most_common(words_per_category * 3)]\n",
    "            \n",
    "            # Filter to words that exist in GloVe\n",
    "            category_words[category_name] = []\n",
    "            for word in most_frequent:\n",
    "                if word in glove_model and len(category_words[category_name]) < words_per_category:\n",
    "                    category_words[category_name].append(word)\n",
    "        \n",
    "        return category_words\n",
    "    \n",
    "    # Extract representative words\n",
    "    print(\"üîç Extracting representative words from each category...\")\n",
    "    category_words = extract_category_words(dataset, categories, words_per_category=10)\n",
    "    \n",
    "    for category, words in category_words.items():\n",
    "        print(f\"\\nüìä {category} category words:\")\n",
    "        print(f\"   {', '.join(words)}\")\n",
    "    \n",
    "    # TODO: Prepare data for visualization by getting embeddings for each word\n",
    "    all_words = []\n",
    "    word_categories = []\n",
    "    word_vectors = []\n",
    "    \n",
    "    for category, words in category_words.items():\n",
    "        for word in words:\n",
    "            all_words.append(word)\n",
    "            word_categories.append(category)\n",
    "            # TODO: Get the embedding vector for this word from glove_model\n",
    "            word_vectors.append(None)  # Replace this line\n",
    "    \n",
    "    print(f\"\\n‚úÖ Prepared {len(all_words)} words for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "if glove_model is not None and len(all_words) > 0:\n",
    "    # Convert to numpy array for processing\n",
    "    embedding_matrix = np.array(word_vectors)\n",
    "    \n",
    "    print(f\"üìä Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    print(\"üîÑ Applying PCA to reduce dimensions...\")\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d_pca = pca.fit_transform(embedding_matrix)\n",
    "    \n",
    "    # Apply t-SNE for non-linear reduction\n",
    "    print(\"üîÑ Applying t-SNE for non-linear reduction...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_words)-1))\n",
    "    embeddings_2d_tsne = tsne.fit_transform(embedding_matrix)\n",
    "    \n",
    "    print(f\"‚úÖ Explained variance by PCA: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Color mapping for categories\n",
    "    colors = {'World': 'red', 'Sports': 'green', 'Business': 'blue', 'Technology': 'purple'}\n",
    "    \n",
    "    # Plot PCA results\n",
    "    for category in category_names:\n",
    "        mask = np.array(word_categories) == category\n",
    "        ax1.scatter(\n",
    "            embeddings_2d_pca[mask, 0], \n",
    "            embeddings_2d_pca[mask, 1],\n",
    "            c=colors[category], \n",
    "            label=category,\n",
    "            alpha=0.7,\n",
    "            s=50\n",
    "        )\n",
    "    \n",
    "    ax1.set_title('Word Embeddings - PCA Projection')\n",
    "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f} variance)')\n",
    "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f} variance)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add word labels for some points\n",
    "    for i, word in enumerate(all_words):\n",
    "        if i % 3 == 0:  # Label every 3rd word to avoid crowding\n",
    "            ax1.annotate(word, (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]), \n",
    "                        xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=8, alpha=0.7)\n",
    "    \n",
    "    # Plot t-SNE results\n",
    "    for category in category_names:\n",
    "        mask = np.array(word_categories) == category\n",
    "        ax2.scatter(\n",
    "            embeddings_2d_tsne[mask, 0], \n",
    "            embeddings_2d_tsne[mask, 1],\n",
    "            c=colors[category], \n",
    "            label=category,\n",
    "            alpha=0.7,\n",
    "            s=50\n",
    "        )\n",
    "    \n",
    "    ax2.set_title('Word Embeddings - t-SNE Projection')\n",
    "    ax2.set_xlabel('t-SNE 1')\n",
    "    ax2.set_ylabel('t-SNE 2')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add word labels for t-SNE\n",
    "    for i, word in enumerate(all_words):\n",
    "        if i % 3 == 0:\n",
    "            ax2.annotate(word, (embeddings_2d_tsne[i, 0], embeddings_2d_tsne[i, 1]), \n",
    "                        xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=8, alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # TODO: Analyze the clustering patterns you observe\n",
    "    print(\"\\nüéØ YOUR CLUSTER ANALYSIS:\")\n",
    "    print(\"   What patterns do you see in the clusters?\")\n",
    "    print(\"   Which categories separate well? Which overlap?\")\n",
    "    print(\"   What does this suggest about semantic relationships?\")\n",
    "    print(\"   YOUR OBSERVATIONS HERE...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb239087",
   "metadata": {},
   "source": [
    "## Part D: Limitations Analysis\n",
    "\n",
    "Analyze vocabulary coverage and identify when custom embeddings might be needed.\n",
    "\n",
    "**Your task**: Complete the coverage analysis calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64843bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset is not None and glove_model is not None:\n",
    "    def analyze_vocabulary_coverage(dataset, model, sample_size=1000):\n",
    "        \"\"\"\n",
    "        Analyze what percentage of words in AG News are covered by GloVe.\n",
    "        \"\"\"\n",
    "        print(f\"üîç Analyzing vocabulary coverage on {sample_size} articles...\")\n",
    "        \n",
    "        # Sample articles for analysis\n",
    "        sample_indices = np.random.choice(len(dataset), size=min(sample_size, len(dataset)), replace=False)\n",
    "        \n",
    "        all_words = []\n",
    "        missing_words = []\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            text = dataset[int(idx)]['text'].lower()\n",
    "            # Simple tokenization\n",
    "            words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "            \n",
    "            for word in words:\n",
    "                if len(word) > 2:  # Skip very short words\n",
    "                    all_words.append(word)\n",
    "                    # TODO: Check if word exists in model vocabulary\n",
    "                    if None:  # Replace this condition\n",
    "                        missing_words.append(word)\n",
    "        \n",
    "        # TODO: Calculate basic statistics\n",
    "        total_words = len(all_words)\n",
    "        unique_words = len(set(all_words))\n",
    "        missing_count = None  # Replace this line\n",
    "        unique_missing = None  # Replace this line\n",
    "        \n",
    "        # TODO: Calculate coverage percentages\n",
    "        coverage_by_tokens = None  # Replace this line\n",
    "        coverage_by_types = None  # Replace this line\n",
    "        \n",
    "        print(f\"\\nüìä Coverage Statistics:\")\n",
    "        print(f\"   Total word tokens: {total_words:,}\")\n",
    "        print(f\"   Unique word types: {unique_words:,}\")\n",
    "        print(f\"   Missing tokens: {missing_count:,}\")\n",
    "        print(f\"   Unique missing: {unique_missing:,}\")\n",
    "        print(f\"\\nüéØ Coverage Rates:\")\n",
    "        print(f\"   By tokens: {coverage_by_tokens:.1f}%\")\n",
    "        print(f\"   By types:  {coverage_by_types:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            'all_words': all_words,\n",
    "            'missing_words': missing_words,\n",
    "            'coverage_tokens': coverage_by_tokens,\n",
    "            'coverage_types': coverage_by_types,\n",
    "            'unique_missing': unique_missing\n",
    "        }\n",
    "    \n",
    "    # Run coverage analysis\n",
    "    coverage_stats = analyze_vocabulary_coverage(dataset, glove_model, sample_size=2000)\n",
    "    \n",
    "    # Analyze types of missing words\n",
    "    missing_words_freq = Counter(coverage_stats['missing_words'])\n",
    "    \n",
    "    print(f\"\\nüîç Most common missing words:\")\n",
    "    for word, count in missing_words_freq.most_common(15):\n",
    "        print(f\"   {word:<15} (appears {count} times)\")\n",
    "    \n",
    "    # Categorize missing words\n",
    "    def categorize_missing_word(word):\n",
    "        if any(char.isdigit() for char in word):\n",
    "            return \"Numbers/Codes\"\n",
    "        elif len(word) <= 3:\n",
    "            return \"Abbreviations\"\n",
    "        elif word.endswith('ing') or word.endswith('ed') or word.endswith('ly'):\n",
    "            return \"Inflected Forms\"\n",
    "        elif word.isupper():\n",
    "            return \"Acronyms\"\n",
    "        elif word[0].isupper():\n",
    "            return \"Proper Names\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    category_counts = Counter()\n",
    "    unique_missing_words = list(set(coverage_stats['missing_words']))\n",
    "    \n",
    "    for word in unique_missing_words:\n",
    "        category = categorize_missing_word(word)\n",
    "        category_counts[category] += 1\n",
    "    \n",
    "    print(f\"\\nüìà Types of missing words:\")\n",
    "    for category, count in category_counts.most_common():\n",
    "        percentage = (count / len(unique_missing_words)) * 100\n",
    "        print(f\"   {category:<15}: {count:3d} words ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # TODO: Provide your analysis\n",
    "    print(f\"\\nü§î YOUR ANALYSIS:\")\n",
    "    print(f\"   What types of words are commonly missing?\")\n",
    "    print(f\"   How might this affect a news recommender system?\")\n",
    "    print(f\"   What recommendations would you make?\")\n",
    "    print(f\"   YOUR INSIGHTS HERE...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ebbe3",
   "metadata": {},
   "source": [
    "## üéì Reflection Questions\n",
    "\n",
    "**Answer these based on your analysis**:\n",
    "\n",
    "1. **Semantic Quality**: How well did GloVe capture relationships relevant to news categorization? What evidence supports your conclusion?\n",
    "\n",
    "   *Your answer here...*\n",
    "\n",
    "2. **Coverage Analysis**: What percentage of vocabulary was covered? What types of words were commonly missing? How would this affect a production system?\n",
    "\n",
    "   *Your answer here...*\n",
    "\n",
    "3. **Clustering Insights**: Which news categories clustered well in the visualization? Which ones overlapped? What does this suggest about their vocabulary similarity?\n",
    "\n",
    "   *Your answer here...*\n",
    "\n",
    "   *Your answer here...*\n",
    "\n",
    "4. **Limitations**: What are the main limitations of this analysis? What additional tests would you run before deploying to production?\n",
    "\n",
    "   *Your answer here...*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
