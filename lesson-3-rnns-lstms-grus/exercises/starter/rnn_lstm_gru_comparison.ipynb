{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85dcf51",
   "metadata": {},
   "source": [
    "# Exercise: Compare RNN, LSTM, and GRU for Sequence Prediction\n",
    "\n",
    "**Scenario**: Your keyboard team has limited compute budget for real-time inference on mobile devices. The product manager wants the smallest model that still handles long-context prediction. \n",
    "\n",
    "**Your Mission**: Benchmark RNN vs. LSTM vs. GRU on prediction accuracy, training speed, and inference latency to make a data-driven recommendation.\n",
    "\n",
    "**Estimated Time**: 18 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement three recurrent architectures with identical configurations\n",
    "- Compare learning curves, prediction quality, and computational efficiency  \n",
    "- Make architecture recommendations based on systematic benchmarking\n",
    "- Understand trade-offs between model complexity and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set device and seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b0e55",
   "metadata": {},
   "source": [
    "## Part A: Implement Three Architectures\n",
    "\n",
    "**Task**: Create three character-level models with **identical specifications**:\n",
    "- `embedding_dim=128`\n",
    "- `hidden_dim=256` \n",
    "- `num_layers=2`\n",
    "- Same prediction head: `Linear(hidden_dim, vocab_size)`\n",
    "\n",
    "**Your Goal**: Ensure fair comparison by matching all hyperparameters except the core recurrent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53085be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation (same as demo)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-v1')\n",
    "text = ' '.join(dataset['train']['text'][:120])\n",
    "text = ''.join(c for c in text if c.isprintable() and c != '\\t')\n",
    "text = text[:60000]\n",
    "\n",
    "# Character mapping\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample characters: {chars[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement CharRNN class\n",
    "class CharRNN(nn.Module):\n",
    "    \"\"\"Character-level RNN using nn.RNN\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        # TODO: Store dimensions and initialize layers\n",
    "        # TODO: self.hidden_dim = ?\n",
    "        # TODO: self.num_layers = ?\n",
    "        # TODO: self.embedding = nn.Embedding(?, ?)\n",
    "        # TODO: self.rnn = nn.RNN(?, ?, ?, batch_first=True)\n",
    "        # TODO: self.output = nn.Linear(?, ?)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # TODO: Pass through embedding\n",
    "        # TODO: Pass through RNN\n",
    "        # TODO: Use final timestep for prediction: rnn_out[:, -1, :]\n",
    "        # TODO: Return logits and new hidden state\n",
    "        pass\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # TODO: Return tensor of zeros with shape (num_layers, batch_size, hidden_dim)\n",
    "        pass\n",
    "\n",
    "# TODO: Implement CharLSTM class  \n",
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"Character-level LSTM using nn.LSTM\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # TODO: Same structure as RNN but with LSTM layer and dropout\n",
    "        # TODO: Add dropout parameter for regularization\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # TODO: Similar to RNN but handle LSTM's (h, c) tuple\n",
    "        # TODO: Apply dropout after embedding and before output\n",
    "        pass\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # TODO: Initialize both h and c for LSTM (return tuple)\n",
    "        pass\n",
    "\n",
    "# TODO: Implement CharGRU class\n",
    "class CharGRU(nn.Module):\n",
    "    \"\"\"Character-level GRU using nn.GRU\"\"\"  \n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # TODO: Same structure as RNN but with GRU layer and dropout\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # TODO: Same as RNN (GRU returns single hidden state)\n",
    "        # TODO: Apply dropout after embedding and before output\n",
    "        pass\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # TODO: Initialize hidden state for GRU\n",
    "        pass\n",
    "\n",
    "print(\"TODO: Complete the three model implementations above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function provided for you\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# TODO: Initialize models and print parameter counts\n",
    "# Uncomment and complete after implementing the model classes above\n",
    "# rnn_model = CharRNN(vocab_size).to(device)\n",
    "# lstm_model = CharLSTM(vocab_size).to(device)  \n",
    "# gru_model = CharGRU(vocab_size).to(device)\n",
    "\n",
    "# print(\"Parameter Comparison:\")\n",
    "# print(f\"RNN:  {count_parameters(rnn_model):,}\")\n",
    "# print(f\"LSTM: {count_parameters(lstm_model):,}\")\n",
    "# print(f\"GRU:  {count_parameters(gru_model):,}\")\n",
    "\n",
    "print(\"TODO: Uncomment and run after completing model implementations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd7697",
   "metadata": {},
   "source": [
    "## Part B: Train and Compare Learning\n",
    "\n",
    "**Task**: Train each model for 20 epochs on the same data split and measure:\n",
    "- Training loss per epoch\n",
    "- Validation loss per epoch  \n",
    "- Training time per epoch\n",
    "- Final perplexity on test set\n",
    "\n",
    "**Your Goal**: Generate learning curves that reveal which architecture learns most efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation function provided for you\n",
    "def prepare_sequences(text, seq_len=50, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"Prepare train/val/test splits\"\"\"\n",
    "    # Convert text to sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(text) - seq_len):\n",
    "        sequence = [char2idx[ch] for ch in text[i:i+seq_len]]\n",
    "        target = char2idx[text[i+seq_len]]\n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    # Create train/validation/test splits (80/10/10)\n",
    "    total_size = len(X)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    \n",
    "    train_X, train_y = X[:train_size], y[:train_size]\n",
    "    val_X, val_y = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "    test_X, test_y = X[train_size+val_size:], y[train_size+val_size:]\n",
    "    \n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y\n",
    "\n",
    "# Prepare data splits\n",
    "train_X, train_y, val_X, val_y, test_X, test_y = prepare_sequences(text)\n",
    "print(f\"Data splits - Train: {train_X.shape}, Val: {val_X.shape}, Test: {test_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper functions provided for you\n",
    "def train_epoch(model, X, y, optimizer, criterion, batch_size=64):\n",
    "    \"\"\"Train for one epoch and return average loss and time\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, len(X) - batch_size, batch_size):\n",
    "        # Get batch\n",
    "        batch_x = X[i:i+batch_size].to(device)\n",
    "        batch_y = y[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = model.init_hidden(batch_x.size(0))\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(batch_x, hidden)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "    return avg_loss, epoch_time\n",
    "\n",
    "def evaluate_model(model, X, y, criterion, batch_size=64):\n",
    "    \"\"\"Evaluate model and return loss\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X) - batch_size, batch_size):\n",
    "            batch_x = X[i:i+batch_size].to(device)\n",
    "            batch_y = y[i:i+batch_size].to(device)\n",
    "            \n",
    "            hidden = model.init_hidden(batch_x.size(0))\n",
    "            outputs, hidden = model(batch_x, hidden)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "    \n",
    "    return total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "\n",
    "print(\"âœ… Training and evaluation helper functions provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train all three models and collect metrics\n",
    "def train_and_compare_models():\n",
    "    \"\"\"Train RNN, LSTM, GRU and collect metrics\"\"\"\n",
    "    \n",
    "    # TODO: Initialize your completed models\n",
    "    models = {\n",
    "        'RNN': None,    # TODO: Initialize CharRNN(vocab_size).to(device)\n",
    "        'LSTM': None,   # TODO: Initialize CharLSTM(vocab_size).to(device)\n",
    "        'GRU': None     # TODO: Initialize CharGRU(vocab_size).to(device)\n",
    "    }\n",
    "    \n",
    "    # TODO: Create optimizers with different learning rates\n",
    "    # Hint: Use lr=0.002 for RNN, lr=0.0005 for LSTM, lr=0.001 for GRU\n",
    "    # Hint: Add weight_decay=1e-5 for LSTM and GRU to prevent overfitting\n",
    "    optimizers = {\n",
    "        'RNN': None,    # TODO: optim.Adam(models['RNN'].parameters(), lr=?)\n",
    "        'LSTM': None,   # TODO: optim.Adam(models['LSTM'].parameters(), lr=?, weight_decay=?)\n",
    "        'GRU': None     # TODO: optim.Adam(models['GRU'].parameters(), lr=?, weight_decay=?)\n",
    "    }\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # TODO: Initialize metrics tracking\n",
    "    metrics = {\n",
    "        'RNN': {'train_losses': [], 'val_losses': [], 'epoch_times': []},\n",
    "        'LSTM': {'train_losses': [], 'val_losses': [], 'epoch_times': []},\n",
    "        'GRU': {'train_losses': [], 'val_losses': [], 'epoch_times': []}\n",
    "    }\n",
    "    \n",
    "    # TODO: Training loop for 20 epochs\n",
    "    # Hint: For each epoch, train each model and collect metrics\n",
    "    # Hint: Use train_epoch() and evaluate_model() functions provided above\n",
    "    # Hint: Use smaller data subsets for faster training: train_X[:8000], val_X[:2000]\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        print(f\"\\nEpoch {epoch+1}/20\")\n",
    "        \n",
    "        for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "            # TODO: Train the model for this epoch\n",
    "            # TODO: Evaluate on validation set  \n",
    "            # TODO: Store metrics\n",
    "            # TODO: Print progress\n",
    "            pass\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# TODO: Run training comparison\n",
    "# Uncomment after completing the function above and model implementations\n",
    "# print(\"ðŸš€ Starting training comparison...\")\n",
    "# metrics = train_and_compare_models()\n",
    "# print(\"âœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e38ca02",
   "metadata": {},
   "source": [
    "## Part C: Evaluate Prediction Quality  \n",
    "\n",
    "**Task**: Compare the models on:\n",
    "- Text generation quality (seed: \"The meaning of\")\n",
    "- Test set perplexity \n",
    "- Inference speed\n",
    "\n",
    "**Your Goal**: Determine which model produces the most coherent text and runs fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038718ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation helper functions provided for you\n",
    "def calculate_perplexity(model, X, y):\n",
    "    \"\"\"Calculate perplexity on test set\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    avg_loss = evaluate_model(model, X, y, criterion)\n",
    "    return np.exp(avg_loss)\n",
    "\n",
    "def measure_inference_speed(model, X_sample, num_runs=100):\n",
    "    \"\"\"Measure inference time\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            hidden = model.init_hidden(1)\n",
    "            _ = model(X_sample, hidden)\n",
    "        end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs * 1000  # Convert to milliseconds\n",
    "    return avg_time\n",
    "\n",
    "print(\"âœ… Evaluation helper functions provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison helper function provided for you  \n",
    "def create_performance_table(metrics, models_dict, test_X, test_y):\n",
    "    \"\"\"Create comprehensive comparison table\"\"\"\n",
    "    comparison_data = {\n",
    "        'Model': [],\n",
    "        'Parameters': [],\n",
    "        'Final Train Loss': [],\n",
    "        'Final Val Loss': [],\n",
    "        'Test Perplexity': [],\n",
    "        'Avg Epoch Time (s)': [],\n",
    "        'Inference Speed (ms)': []\n",
    "    }\n",
    "    \n",
    "    # Sample for inference speed testing\n",
    "    X_sample = test_X[:1].to(device)\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        comparison_data['Model'].append(model_name)\n",
    "        comparison_data['Parameters'].append(f\"{count_parameters(model):,}\")\n",
    "        comparison_data['Final Train Loss'].append(f\"{metrics[model_name]['train_losses'][-1]:.4f}\")\n",
    "        comparison_data['Final Val Loss'].append(f\"{metrics[model_name]['val_losses'][-1]:.4f}\")\n",
    "        comparison_data['Test Perplexity'].append(f\"{calculate_perplexity(model, test_X[:1000], test_y[:1000]):.2f}\")\n",
    "        comparison_data['Avg Epoch Time (s)'].append(f\"{np.mean(metrics[model_name]['epoch_times']):.1f}\")\n",
    "        comparison_data['Inference Speed (ms)'].append(f\"{measure_inference_speed(model, X_sample):.2f}\")\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# TODO: Create and display performance table after training\n",
    "# Uncomment after completing training\n",
    "# models_dict = {'RNN': rnn_model, 'LSTM': lstm_model, 'GRU': gru_model}\n",
    "# performance_table = create_performance_table(metrics, models_dict, test_X, test_y)\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"ðŸ“Š COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "# print(\"=\"*80)\n",
    "# print(performance_table.to_string(index=False))\n",
    "\n",
    "print(\"TODO: Uncomment and run after completing training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92859aaa",
   "metadata": {},
   "source": [
    "## Part D: Visualization & Analysis\n",
    "\n",
    "Create learning curves and make your recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fe97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function provided for you\n",
    "def plot_learning_curves(metrics):\n",
    "    \"\"\"Plot training and validation curves for all models\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    colors = {'RNN': 'red', 'LSTM': 'blue', 'GRU': 'orange'}\n",
    "    \n",
    "    # Plot training losses\n",
    "    ax1.set_title('Training Loss Over Time')\n",
    "    for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "        ax1.plot(metrics[model_name]['train_losses'], \n",
    "                label=model_name, color=colors[model_name], linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot validation losses  \n",
    "    ax2.set_title('Validation Loss Over Time')\n",
    "    for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "        ax2.plot(metrics[model_name]['val_losses'], \n",
    "                label=model_name, color=colors[model_name], linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Validation Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot epoch times\n",
    "    ax3.set_title('Training Time per Epoch')\n",
    "    for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "        ax3.plot(metrics[model_name]['epoch_times'], \n",
    "                label=model_name, color=colors[model_name], linewidth=2)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Time (seconds)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Create summary comparison\n",
    "    ax4.set_title('Final Performance Summary')\n",
    "    models = ['RNN', 'LSTM', 'GRU']\n",
    "    final_val_losses = [metrics[name]['val_losses'][-1] for name in models]\n",
    "    avg_times = [np.mean(metrics[name]['epoch_times']) for name in models]\n",
    "    \n",
    "    bars = ax4.bar(models, final_val_losses, color=[colors[name] for name in models])\n",
    "    ax4.set_ylabel('Final Validation Loss')\n",
    "    \n",
    "    # Add time annotations on bars\n",
    "    for i, (bar, time_val) in enumerate(zip(bars, avg_times)):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{time_val:.1f}s/epoch', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Generate plots after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513264e8",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What you should have discovered**:\n",
    "\n",
    "1. **LSTM vs GRU**: Similar performance, but GRU is more efficient\n",
    "2. **Vanilla RNN**: Fastest but struggles with long sequences\n",
    "3. **Parameter Trade-off**: More parameters â‰  always better\n",
    "4. **Use Case Matters**: Mobile needs different trade-offs than servers\n",
    "\n",
    "**Next Steps**:\n",
    "- Try different sequence lengths to find breaking points\n",
    "- Experiment with different hidden dimensions\n",
    "- Test on other datasets (code, music, time series)\n",
    "- Implement attention mechanisms for even longer sequences"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
