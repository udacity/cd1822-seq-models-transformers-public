{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 Exercise: Improve Q&A Accuracy with Attention Layers\n",
    "\n",
    "**Estimated Time:** 15 minutes\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Lesson 4, you built a Q&A system but discovered the **context vector bottleneck** - long contexts hurt performance. Today, you'll fix this by implementing attention mechanisms!\n",
    "\n",
    "## What You'll Do\n",
    "\n",
    "1. **Part A:** Implement `AttentionLayer` class (compute attention weights)\n",
    "2. **Part B:** Implement `DecoderWithAttention` (integrate attention into decoder)\n",
    "3. **Part C:** Train your model and compare to baseline\n",
    "4. **Part D:** Visualize attention heatmaps\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Import data utilities (from Lesson 4)\n",
    "from data import (\n",
    "    SyntheticQAGenerator,\n",
    "    build_vocabulary,\n",
    "    encode_text,\n",
    "    decode_text,\n",
    "    PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, SEP_TOKEN\n",
    ")\n",
    "\n",
    "# Import model classes (you'll implement the attention parts!)\n",
    "from models import (\n",
    "    Encoder,  # From Lesson 4 - provided\n",
    "    AttentionLayer,  # NEW - YOU implement!\n",
    "    DecoderWithAttention,  # NEW - YOU implement!\n",
    "    Seq2SeqWithAttention,  # NEW - YOU implement!\n",
    "    count_parameters\n",
    ")\n",
    "\n",
    "# Import visualization\n",
    "from attention_viz import plot_attention, highlight_max_attention\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"\\n‚úì Imports successful!\")\n",
    "print(\"\\n‚ö†Ô∏è  Remember: You need to implement the attention classes in models.py!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Implement Attention Mechanism\n",
    "\n",
    "Your first task is to implement the `AttentionLayer` class in `models.py`.\n",
    "\n",
    "## What Attention Does\n",
    "\n",
    "```\n",
    "Without Attention:\n",
    "  Encoder: [h1, h2, h3, ..., hN] ‚Üí Only use hN (bottleneck!)\n",
    "\n",
    "With Attention:\n",
    "  Encoder: [h1, h2, h3, ..., hN] ‚Üí Use ALL states!\n",
    "  \n",
    "  At each decoder step:\n",
    "    1. Compute scores for each encoder state\n",
    "    2. Apply softmax ‚Üí attention weights\n",
    "    3. Weighted sum ‚Üí context vector\n",
    "```\n",
    "\n",
    "## Your Task\n",
    "\n",
    "Open `models.py` and complete the `AttentionLayer` class:\n",
    "\n",
    "1. Initialize layers in `__init__`\n",
    "2. Implement the `forward` method following the TODOs\n",
    "\n",
    "## Test Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING YOUR AttentionLayer IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Test AttentionLayer\n",
    "    attn = AttentionLayer(hidden_dim=256)\n",
    "    \n",
    "    # Create test inputs\n",
    "    batch_size = 4\n",
    "    src_len = 20\n",
    "    hidden_dim = 256\n",
    "    \n",
    "    test_hidden = torch.randn(batch_size, hidden_dim)\n",
    "    test_encoder = torch.randn(batch_size, src_len, hidden_dim * 2)\n",
    "    \n",
    "    # Forward pass\n",
    "    context, weights = attn(test_hidden, test_encoder)\n",
    "    \n",
    "    # Check shapes\n",
    "    print(\"\\nShape Checks:\")\n",
    "    print(f\"  Context shape: {context.shape} (expected: [{batch_size}, {hidden_dim*2}])\")\n",
    "    print(f\"  Weights shape: {weights.shape} (expected: [{batch_size}, {src_len}])\")\n",
    "    \n",
    "    assert context.shape == (batch_size, hidden_dim * 2), \\\n",
    "        f\"‚ùå Context shape is {context.shape}, expected ({batch_size}, {hidden_dim*2})\"\n",
    "    assert weights.shape == (batch_size, src_len), \\\n",
    "        f\"‚ùå Weights shape is {weights.shape}, expected ({batch_size}, {src_len})\"\n",
    "    \n",
    "    print(\"  ‚úì Shapes correct!\")\n",
    "    \n",
    "    # Check that weights sum to 1\n",
    "    weight_sums = weights.sum(dim=1)\n",
    "    print(f\"\\nAttention Weight Sums: {weight_sums}\")\n",
    "    assert torch.allclose(weight_sums, torch.ones(batch_size), atol=1e-6), \\\n",
    "        f\"‚ùå Attention weights don't sum to 1.0! Got: {weight_sums}\"\n",
    "    \n",
    "    print(\"  ‚úì Weights sum to 1.0!\")\n",
    "    \n",
    "    # Check that weights are non-negative\n",
    "    assert (weights >= 0).all(), \"‚ùå Attention weights contain negative values!\"\n",
    "    print(\"  ‚úì All weights non-negative!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ SUCCESS! Your AttentionLayer is working correctly!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except NotImplementedError as e:\n",
    "    print(\"\\n‚ùå AttentionLayer not implemented yet!\")\n",
    "    print(\"   Go to models.py and complete the TODOs in the AttentionLayer class.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error in AttentionLayer: {e}\")\n",
    "    print(\"   Check your implementation against the hints in models.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Integrate Attention into Decoder\n",
    "\n",
    "Now implement `DecoderWithAttention` and `Seq2SeqWithAttention` in `models.py`.\n",
    "\n",
    "## Key Changes from Lesson 4\n",
    "\n",
    "**Lesson 4 Decoder:**\n",
    "```python\n",
    "def forward(x, hidden, cell):\n",
    "    embedded = self.embedding(x)\n",
    "    output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "    return output, hidden, cell\n",
    "```\n",
    "\n",
    "**Lesson 5 Decoder (with Attention):**\n",
    "```python\n",
    "def forward(x, hidden, cell, encoder_outputs):  # ‚Üê Extra parameter!\n",
    "    embedded = self.embedding(x)\n",
    "    \n",
    "    # NEW: Compute attention\n",
    "    context, attn_weights = self.attention(hidden, encoder_outputs)\n",
    "    \n",
    "    # NEW: Concatenate with embedding\n",
    "    lstm_input = torch.cat([embedded, context], dim=2)\n",
    "    \n",
    "    output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "    return output, hidden, cell, attn_weights  # ‚Üê Return attention!\n",
    "```\n",
    "\n",
    "## Test Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING YOUR DecoderWithAttention IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Test DecoderWithAttention\n",
    "    vocab_size = 100\n",
    "    embed_dim = 128\n",
    "    hidden_dim = 256\n",
    "    \n",
    "    decoder = DecoderWithAttention(vocab_size, embed_dim, hidden_dim)\n",
    "    \n",
    "    # Create test inputs\n",
    "    batch_size = 4\n",
    "    src_len = 20\n",
    "    \n",
    "    test_input = torch.randint(0, vocab_size, (batch_size, 1))\n",
    "    test_hidden = torch.randn(1, batch_size, hidden_dim)\n",
    "    test_cell = torch.randn(1, batch_size, hidden_dim)\n",
    "    test_encoder_outputs = torch.randn(batch_size, src_len, hidden_dim * 2)\n",
    "    \n",
    "    # Forward pass\n",
    "    pred, h, c, attn = decoder(test_input, test_hidden, test_cell, test_encoder_outputs)\n",
    "    \n",
    "    # Check shapes\n",
    "    print(\"\\nShape Checks:\")\n",
    "    print(f\"  Prediction shape: {pred.shape} (expected: [{batch_size}, 1, {vocab_size}])\")\n",
    "    print(f\"  Attention shape:  {attn.shape} (expected: [{batch_size}, {src_len}])\")\n",
    "    \n",
    "    assert pred.shape == (batch_size, 1, vocab_size), \\\n",
    "        f\"‚ùå Prediction shape is {pred.shape}, expected ({batch_size}, 1, {vocab_size})\"\n",
    "    assert attn.shape == (batch_size, src_len), \\\n",
    "        f\"‚ùå Attention shape is {attn.shape}, expected ({batch_size}, {src_len})\"\n",
    "    \n",
    "    print(\"  ‚úì Shapes correct!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ SUCCESS! Your DecoderWithAttention is working correctly!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except NotImplementedError:\n",
    "    print(\"\\n‚ùå DecoderWithAttention not implemented yet!\")\n",
    "    print(\"   Go to models.py and complete the TODOs.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error in DecoderWithAttention: {e}\")\n",
    "    print(\"   Check your implementation against the hints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Train and Compare\n",
    "\n",
    "Now let's train your attention model and compare it to the Lesson 4 baseline!\n",
    "\n",
    "## Load Data (Same as Lesson 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "# Generate same dataset as Lesson 4\n",
    "generator = SyntheticQAGenerator(seed=42)\n",
    "qa_data, context_lengths = generator.generate_dataset(\n",
    "    n_short=400,\n",
    "    n_medium=400,\n",
    "    n_long=400,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab, idx2word = build_vocabulary(qa_data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"‚úì Generated {len(qa_data)} Q&A pairs\")\n",
    "print(f\"‚úì Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.70 * len(qa_data))\n",
    "val_size = int(0.20 * len(qa_data))\n",
    "\n",
    "train_data = qa_data[:train_size]\n",
    "val_data = qa_data[train_size:train_size+val_size]\n",
    "test_data = qa_data[train_size+val_size:]\n",
    "\n",
    "print(f\"\\nSplits: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (same as Lesson 4)\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, vocab, idx2word, max_len=100):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.idx2word = idx2word\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ctx, q, ans = self.data[idx]\n",
    "        \n",
    "        ctx_ids = encode_text(ctx, self.vocab)\n",
    "        q_ids = encode_text(q, self.vocab)\n",
    "        ans_ids = encode_text(ans, self.vocab)\n",
    "        \n",
    "        enc_in = (ctx_ids + [SEP_TOKEN] + q_ids + [EOS_TOKEN])[:self.max_len]\n",
    "        enc_in += [PAD_TOKEN] * (self.max_len - len(enc_in))\n",
    "        \n",
    "        dec_in = ([SOS_TOKEN] + ans_ids)[:self.max_len]\n",
    "        dec_in += [PAD_TOKEN] * (self.max_len - len(dec_in))\n",
    "        \n",
    "        dec_out = (ans_ids + [EOS_TOKEN])[:self.max_len]\n",
    "        dec_out += [PAD_TOKEN] * (self.max_len - len(dec_out))\n",
    "        \n",
    "        return {\n",
    "            'encoder_input': torch.LongTensor(enc_in),\n",
    "            'decoder_input': torch.LongTensor(dec_in),\n",
    "            'decoder_target': torch.LongTensor(dec_out),\n",
    "            'context_length': len(ctx_ids)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'encoder_input': torch.stack([item['encoder_input'] for item in batch]),\n",
    "        'decoder_input': torch.stack([item['decoder_input'] for item in batch]),\n",
    "        'decoder_target': torch.stack([item['decoder_target'] for item in batch]),\n",
    "        'context_lengths': torch.tensor([item['context_length'] for item in batch])\n",
    "    }\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = QADataset(train_data, vocab, idx2word)\n",
    "val_dataset = QADataset(val_data, vocab, idx2word)\n",
    "test_dataset = QADataset(test_data, vocab, idx2word)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"‚úì DataLoaders ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with YOUR Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BUILDING MODEL WITH YOUR ATTENTION IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    EMBED_DIM = 128\n",
    "    HIDDEN_DIM = 256\n",
    "    \n",
    "    # Create encoder (from Lesson 4)\n",
    "    encoder = Encoder(vocab_size, EMBED_DIM, HIDDEN_DIM, padding_idx=PAD_TOKEN).to(device)\n",
    "    \n",
    "    # Create decoder with YOUR attention\n",
    "    attention_decoder = DecoderWithAttention(\n",
    "        vocab_size, EMBED_DIM, HIDDEN_DIM, padding_idx=PAD_TOKEN\n",
    "    ).to(device)\n",
    "    \n",
    "    # Complete model\n",
    "    model = Seq2SeqWithAttention(encoder, attention_decoder).to(device)\n",
    "    \n",
    "    num_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n‚úì Model created successfully!\")\n",
    "    print(f\"\\nArchitecture:\")\n",
    "    print(f\"  Embedding:  {EMBED_DIM}D\")\n",
    "    print(f\"  Hidden:     {HIDDEN_DIM}D\")\n",
    "    print(f\"  Parameters: {num_params:,}\")\n",
    "    print(f\"\\n‚≠ê Your attention mechanism is integrated!\")\n",
    "    \n",
    "except NotImplementedError:\n",
    "    print(\"\\n‚ùå Seq2SeqWithAttention not implemented yet!\")\n",
    "    print(\"   Complete the TODOs in models.py first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Model (~3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        src = batch['encoder_input'].to(device)\n",
    "        trg_in = batch['decoder_input'].to(device)\n",
    "        trg_out = batch['decoder_target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg_in, teacher_forcing_ratio=0.5)\n",
    "        \n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        trg_out = trg_out.reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg_out)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['encoder_input'].to(device)\n",
    "            trg_in = batch['decoder_input'].to(device)\n",
    "            trg_out = batch['decoder_target'].to(device)\n",
    "            \n",
    "            output, _ = model(src, trg_in, teacher_forcing_ratio=0)\n",
    "            \n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            trg_out = trg_out.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg_out)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Train!\n",
    "NUM_EPOCHS = 25\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING YOUR ATTENTION MODEL (~3 minutes)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Epoch | Train Loss | Val Loss\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:8.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")\n",
    "print(f\"\\nFinal: Train {train_losses[-1]:.3f}, Val {val_losses[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Your Attention Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_attention(model, context, question, vocab, idx2word, max_len=20):\n",
    "    \"\"\"Generate answer using your attention model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ctx_ids = encode_text(context, vocab)\n",
    "        q_ids = encode_text(question, vocab)\n",
    "        src_ids = (ctx_ids + [SEP_TOKEN] + q_ids + [EOS_TOKEN])[:100]\n",
    "        src_ids += [PAD_TOKEN] * (100 - len(src_ids))\n",
    "        src = torch.LongTensor([src_ids]).to(device)\n",
    "        \n",
    "        # Encode\n",
    "        embedded = model.encoder.embedding(src)\n",
    "        encoder_outputs, (hidden, cell) = model.encoder.lstm(embedded)\n",
    "        \n",
    "        hidden = torch.tanh(\n",
    "            model.encoder.fc_hidden(torch.cat([hidden[-2], hidden[-1]], dim=1))\n",
    "        ).unsqueeze(0)\n",
    "        cell = torch.tanh(\n",
    "            model.encoder.fc_cell(torch.cat([cell[-2], cell[-1]], dim=1))\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(device)\n",
    "        result = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell, _ = model.decoder(\n",
    "                decoder_input, hidden, cell, encoder_outputs\n",
    "            )\n",
    "            pred_token = output.argmax(2).item()\n",
    "            \n",
    "            if pred_token == EOS_TOKEN:\n",
    "                break\n",
    "            \n",
    "            result.append(pred_token)\n",
    "            decoder_input = torch.LongTensor([[pred_token]]).to(device)\n",
    "        \n",
    "        return decode_text(result, idx2word)\n",
    "\n",
    "# Test by context length\n",
    "print(\"\\nEvaluating by context length...\")\n",
    "results_by_length = []\n",
    "\n",
    "for ctx, q, ref in tqdm(test_data, desc=\"Testing\"):\n",
    "    ctx_len = len(ctx.split())\n",
    "    pred = generate_answer_with_attention(model, ctx, q, vocab, idx2word)\n",
    "    correct = 1 if pred.strip().lower() == ref.strip().lower() else 0\n",
    "    results_by_length.append((ctx_len, correct))\n",
    "\n",
    "# Group by length\n",
    "length_buckets = {\n",
    "    'Short (8-12 words)': [],\n",
    "    'Medium (25-35 words)': [],\n",
    "    'Long (50-70 words)': []\n",
    "}\n",
    "\n",
    "for ctx_len, correct in results_by_length:\n",
    "    if ctx_len <= 14:\n",
    "        length_buckets['Short (8-12 words)'].append(correct)\n",
    "    elif 22 <= ctx_len <= 38:\n",
    "        length_buckets['Medium (25-35 words)'].append(correct)\n",
    "    elif ctx_len >= 45:\n",
    "        length_buckets['Long (50-70 words)'].append(correct)\n",
    "\n",
    "# Calculate accuracies\n",
    "your_results = {}\n",
    "for label, scores in length_buckets.items():\n",
    "    if scores:\n",
    "        your_results[label] = np.mean(scores) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to baseline\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: Baseline (Lesson 4) vs. Your Attention Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_results = {\n",
    "    'Short (8-12 words)': 90.0,\n",
    "    'Medium (25-35 words)': 75.0,\n",
    "    'Long (50-70 words)': 55.0\n",
    "}\n",
    "\n",
    "print(\"\\nResults by Context Length:\")\n",
    "print(f\"{'Length':<25} {'Baseline':<12} {'Your Model':<12} {'Improvement'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for label in ['Short (8-12 words)', 'Medium (25-35 words)', 'Long (50-70 words)']:\n",
    "    baseline = baseline_results[label]\n",
    "    yours = your_results[label]\n",
    "    diff = yours - baseline\n",
    "    symbol = \"‚úì\" if diff > 5 else \"\"\n",
    "    print(f\"{label:<25} {baseline:>6.1f}%     {yours:>6.1f}%     {diff:>+5.1f}% {symbol}\")\n",
    "\n",
    "# Calculate improvement on long contexts\n",
    "long_improvement = your_results['Long (50-70 words)'] - baseline_results['Long (50-70 words)']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if long_improvement >= 25:\n",
    "    print(\"üéâ EXCELLENT! Your attention implementation works great!\")\n",
    "    print(f\"   Long context improvement: +{long_improvement:.0f} points!\")\n",
    "    print(\"   Bottleneck eliminated! ‚úì\")\n",
    "elif long_improvement >= 15:\n",
    "    print(\"‚úì Good! Your attention helps, but could be better.\")\n",
    "    print(f\"   Long context improvement: +{long_improvement:.0f} points\")\n",
    "    print(\"   Check your implementation for any issues.\")\n",
    "else:\n",
    "    print(\"‚ö† Something might be wrong with your implementation.\")\n",
    "    print(f\"   Long context improvement: +{long_improvement:.0f} points (expected 25-30)\")\n",
    "    print(\"   Review your attention code carefully.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Baseline\n",
    "labels_short = ['Short', 'Medium', 'Long']\n",
    "baseline_vals = [baseline_results[k] for k in baseline_results.keys()]\n",
    "axes[0].bar(labels_short, baseline_vals,\n",
    "            color=['#2ecc71', '#f39c12', '#e74c3c'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Baseline (Without Attention)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(baseline_vals):\n",
    "    axes[0].text(i, v + 2, f'{v:.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Your model\n",
    "your_vals = [your_results[k] for k in your_results.keys()]\n",
    "colors = ['#27ae60' if v >= 85 else '#16a085' if v >= 70 else '#e67e22' for v in your_vals]\n",
    "axes[1].bar(labels_short, your_vals,\n",
    "            color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Your Model (With Attention)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(your_vals):\n",
    "    axes[1].text(i, v + 2, f'{v:.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Did Your Attention Work?', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part D: Visualize Attention\n",
    "\n",
    "Let's see WHERE your model looks when generating answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING YOUR ATTENTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find a good long-context example\n",
    "for ctx, q, ans in test_data:\n",
    "    if len(ctx.split()) > 40 and len(ans.split()) <= 3:\n",
    "        example_ctx = ctx\n",
    "        example_q = q\n",
    "        example_ans = ans\n",
    "        break\n",
    "\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Context:  {example_ctx}\")\n",
    "print(f\"Question: {example_q}\")\n",
    "print(f\"Answer:   {example_ans}\")\n",
    "\n",
    "# Generate and collect attention\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ctx_ids = encode_text(example_ctx, vocab)\n",
    "    q_ids = encode_text(example_q, vocab)\n",
    "    ans_ids = encode_text(example_ans, vocab)\n",
    "    \n",
    "    src_ids = (ctx_ids + [SEP_TOKEN] + q_ids + [EOS_TOKEN])[:100]\n",
    "    src_ids += [PAD_TOKEN] * (100 - len(src_ids))\n",
    "    src = torch.LongTensor([src_ids]).to(device)\n",
    "    \n",
    "    trg_ids = [SOS_TOKEN] + ans_ids + [EOS_TOKEN]\n",
    "    trg_ids += [PAD_TOKEN] * (100 - len(trg_ids))\n",
    "    trg = torch.LongTensor([trg_ids[:100]]).to(device)\n",
    "    \n",
    "    outputs, attentions = model(src, trg, teacher_forcing_ratio=0)\n",
    "    \n",
    "    attn = attentions[0].cpu().numpy()\n",
    "    \n",
    "    src_words = example_ctx.split() + ['<SEP>'] + example_q.split()\n",
    "    ans_words = example_ans.split()\n",
    "    \n",
    "    attn = attn[:len(ans_words), :len(src_words)]\n",
    "\n",
    "# Plot heatmap\n",
    "fig = plot_attention(\n",
    "    attn,\n",
    "    src_words,\n",
    "    ans_words,\n",
    "    title=\"Your Attention Model: Where Does It Look?\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Text summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ATTENTION FOCUS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "highlight_max_attention(attn, src_words, ans_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Questions\n",
    "\n",
    "Look at your attention heatmap and answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ANALYSIS QUESTIONS:\n",
    "\n",
    "1. Does your model focus on relevant words?\n",
    "   - When generating the answer, where is attention highest?\n",
    "   - Does it look at the right parts of the context?\n",
    "\n",
    "2. Does it ignore irrelevant words?\n",
    "   - Are filler words getting low attention?\n",
    "   - Is the model \"smart\" about what to focus on?\n",
    "\n",
    "3. How does this eliminate the bottleneck?\n",
    "   - In Lesson 4, the model forgot early context\n",
    "   - Can your model now look back at ANY position?\n",
    "\n",
    "TODO: Write your analysis below (2-3 sentences)\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Add your analysis here\n",
    "your_analysis = \"\"\"\n",
    "YOUR ANALYSIS HERE:\n",
    "\n",
    "I notice that...\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(your_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What You Implemented\n",
    "\n",
    "1. ‚úÖ **AttentionLayer** - Computes attention weights and context vectors\n",
    "2. ‚úÖ **DecoderWithAttention** - Integrates attention into decoder\n",
    "3. ‚úÖ **Seq2SeqWithAttention** - Complete model with attention\n",
    "\n",
    "## What You Discovered\n",
    "\n",
    "- **Before (Lesson 4):** Long contexts = 55% accuracy (bottleneck!)\n",
    "- **After (Your attention):** Long contexts = ~85% accuracy (+30 points!)\n",
    "- **Visualization:** Your model \"looks\" at relevant words\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "**Attention eliminates the fixed-size bottleneck** by letting the decoder dynamically focus on any part of the input. This breakthrough:\n",
    "- Made long sequences practical\n",
    "- Led to Transformers (\"Attention is All You Need\")\n",
    "- Enabled modern LLMs (BERT, GPT, Claude!)\n",
    "\n",
    "üéâ **Congratulations! You've implemented a fundamental breakthrough in deep learning!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
