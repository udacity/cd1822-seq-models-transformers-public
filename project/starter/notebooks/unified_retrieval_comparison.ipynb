{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3782c561",
   "metadata": {},
   "source": [
    "# Information Retrieval: From Keywords to Transformers\n",
    "\n",
    "## Educational Demonstration: Traditional vs Semantic Search\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the limitations of keyword-based retrieval (BM25)\n",
    "- Explore how transformers enable semantic understanding\n",
    "- Compare retrieval performance with real-world queries\n",
    "- Demonstrate the evolution from sequence models to transformers\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Traditional IR**: Keyword matching, TF-IDF, BM25\n",
    "- **Sequence Models**: RNNs, LSTMs for text understanding\n",
    "- **Transformers**: Self-attention, contextual embeddings\n",
    "- **Semantic Search**: Dense embeddings, cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cde323",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59810215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress verbose logging from gensim and word2vec\n",
    "logging.getLogger('gensim').setLevel(logging.WARNING)\n",
    "logging.getLogger('word2vec_retriever').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Suppress gensim warnings and exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gensim\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*gensim.models.word2vec_inner.*\")\n",
    "\n",
    "# Add src to path for our utilities\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Our utility modules\n",
    "from data_loader import DataLoader\n",
    "from bm25_retriever import BM25Retriever\n",
    "from transformer_retriever import TransformerRetriever\n",
    "from evaluator import IRMetrics\n",
    "from utils import create_comparison_visualization, print_comparison_table\n",
    "\n",
    "print(\"‚úÖ Libraries and utilities loaded\")\n",
    "print(\"üìö Ready to explore traditional vs semantic retrieval!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0495ff",
   "metadata": {},
   "source": [
    "## üì• Load Natural Questions Dataset\n",
    "\n",
    "We'll use the **Natural Questions** dataset - real Google search queries with Wikipedia answers.\n",
    "This provides a realistic evaluation of how different retrieval methods handle real-world information needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a14c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Loading Natural Questions dataset with smart sampling...\\n\")\n",
    "\n",
    "# Suppress verbose logging from data_loader\n",
    "import logging\n",
    "logging.getLogger('data_loader').setLevel(logging.WARNING)\n",
    "\n",
    "# Load with manageable sample for educational demonstration\n",
    "loader = DataLoader(\"BeIR/nq\")\n",
    "dataset = loader.load_dataset(split=\"test\", query_sample_size=2000, random_seed=42)\n",
    "corpus_texts, query_texts, qrels_dict = loader.prepare_retrieval_data()\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"  üìÑ Documents: {len(corpus_texts):,}\")\n",
    "print(f\"  ‚ùì Queries: {len(query_texts):,}\")\n",
    "print(f\"  üîó Query-document pairs: {len(qrels_dict):,}\")\n",
    "\n",
    "# Show examples to understand the task\n",
    "print(f\"\\nüí° Example Query-Document Pair:\")\n",
    "print(f\"  Query: '{query_texts[0]}'\")\n",
    "if 0 in qrels_dict:\n",
    "    rel_doc_id = list(qrels_dict[0].keys())[0]\n",
    "    print(f\"  Relevant doc: '{corpus_texts[rel_doc_id][:120]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee71b2",
   "metadata": {},
   "source": [
    "## üìä Evaluation Metrics Explained\n",
    "\n",
    "**Recall@k**: Found relevant docs / Total relevant docs  \n",
    "*Example: 3 found out of 5 relevant ‚Üí Recall@5 = 60%*  \n",
    "**Why**: Measures completeness - did we miss important information?\n",
    "\n",
    "**Precision@k**: Relevant docs in top-k / k  \n",
    "*Example: 3 relevant out of 5 returned ‚Üí Precision@5 = 60%*  \n",
    "**Why**: Measures quality - are results actually useful?\n",
    "\n",
    "**MRR (Mean Reciprocal Rank)**: Average of 1/rank of first relevant doc  \n",
    "*Example: First relevant at position 2 ‚Üí RR = 0.5*  \n",
    "**Why**: Measures efficiency - how fast do users find answers?\n",
    "\n",
    "**k values**: k=1 (mobile), k=5 (above fold), k=10 (max users check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab6bfa",
   "metadata": {},
   "source": [
    "# Retrieval Method Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c3bf5",
   "metadata": {},
   "source": [
    "##  Method 1: Traditional Keyword-Based Retrieval (BM25)\n",
    "\n",
    "**The Challenge with Keywords:**\n",
    "- Exact word matching only\n",
    "- No understanding of synonyms or context\n",
    "- Struggles with semantic similarity\n",
    "\n",
    "**How BM25 Works:**\n",
    "1. **Term Frequency (TF)**: How often does a word appear in a document?\n",
    "2. **Inverse Document Frequency (IDF)**: How rare is the word across the corpus?\n",
    "3. **Document Length Normalization**: Adjust for document length differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî§ Building BM25 Index...\")\n",
    "\n",
    "# Initialize and build BM25 index\n",
    "bm25_retriever = BM25Retriever()\n",
    "bm25_retriever.build_index(corpus_texts)\n",
    "\n",
    "# Run retrieval\n",
    "bm25_results = bm25_retriever.retrieve(query_texts, k=20)\n",
    "\n",
    "# Evaluate performance\n",
    "bm25_metrics = IRMetrics.evaluate_retrieval(bm25_results, qrels_dict)\n",
    "IRMetrics.print_metrics(bm25_metrics, \"BM25 (Keyword-Based) Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd42cf",
   "metadata": {},
   "source": [
    "## üìä Method 2: Word2Vec Static Embeddings\n",
    "\n",
    "**The Bridge Between Keywords and Context:**\n",
    "- **Static word embeddings**: Each word has one fixed vector representation\n",
    "- **Limitations**: No context awareness - \"bank\" always has the same vector in the example of 'I deposit my money into bank' vs. 'I like to gaze to stars at the bank of the river'\n",
    "\n",
    "**How Word2Vec Works:**\n",
    "1. **Training**: Learn word relationships from text corpus using context windows\n",
    "2. **Word Vectors**: Map words to dense vector space (e.g., 100-300 dimensions)\n",
    "\n",
    "**Key Insight**: This represents the **pre-transformer era** of semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b245aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Building Word2Vec Index...\")\n",
    "\n",
    "\n",
    "# Reload the Word2Vec retriever module to get latest changes\n",
    "import importlib\n",
    "if 'word2vec_retriever' in sys.modules:\n",
    "    importlib.reload(sys.modules['word2vec_retriever'])\n",
    "\n",
    "# Import our Word2Vec retriever\n",
    "from word2vec_retriever import Word2VecRetriever\n",
    "\n",
    "# Initialize Word2Vec retriever with improved parameters\n",
    "word2vec_retriever = Word2VecRetriever(vector_size=200, window=10, min_count=1)\n",
    "word2vec_retriever.build_index(corpus_texts)\n",
    "\n",
    "# Show vocabulary statistics\n",
    "stats = word2vec_retriever.get_vocabulary_stats()\n",
    "print(f\"üìö Word2Vec vocabulary: {stats['vocabulary_size']:,} words\")\n",
    "print(f\"üìè Vector size: {word2vec_retriever.vector_size}\")\n",
    "print(f\"ü™ü Context window: {word2vec_retriever.window}\")\n",
    "\n",
    "# Run retrieval\n",
    "word2vec_results = word2vec_retriever.retrieve(query_texts, k=20)\n",
    "\n",
    "# Evaluate performance\n",
    "word2vec_metrics = IRMetrics.evaluate_retrieval(word2vec_results, qrels_dict)\n",
    "IRMetrics.print_metrics(word2vec_metrics, \"Word2Vec (Static Embeddings) Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b291b",
   "metadata": {},
   "source": [
    "### üîß Word2Vec Parameter Optimization\n",
    "\n",
    "**The Challenge with Word2Vec:**\n",
    "The current Word2Vec performance is suboptimal. This is common when:\n",
    "- Training corpus is small.\n",
    "- Default parameters aren't optimized for the specific task\n",
    "- Model architecture choices aren't ideal for retrieval\n",
    "\n",
    "**Parameters to Optimize:**\n",
    "1. **`vector_size`**: Embedding dimensions (100, 200, 300)\n",
    "2. **`window`**: Context window size (5, 10, 15 words)\n",
    "3. **`min_count`**: Minimum word frequency threshold (1, 2)\n",
    "4. **`sg`**: Algorithm choice (0=CBOW, 1=Skip-gram)\n",
    "5. **`epochs`**: Training iterations (10, 20)\n",
    "\n",
    "**Strategy:**\n",
    "- Quick grid search with limited combinations\n",
    "- Focus on MRR improvement as primary metric\n",
    "- Update global variables with best configuration\n",
    "\n",
    "**Educational Note:** This demonstrates why embeddings often need task-specific tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reload module if needed\n",
    "import importlib\n",
    "if 'word2vec_retriever' in sys.modules:\n",
    "    importlib.reload(sys.modules['word2vec_retriever'])\n",
    "\n",
    "from word2vec_retriever import Word2VecRetriever\n",
    "from evaluator import IRMetrics\n",
    "from utils import random_param_combinations\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'vector_size': [100, 200, 300],\n",
    "    'window': [5, 20],\n",
    "    'min_count': [1, 2],\n",
    "    'sg': [0, 1],\n",
    "    'epochs': [25, 100]\n",
    "}\n",
    "\n",
    "param_combinations = random_param_combinations(param_grid, n_samples=10)\n",
    "\n",
    "\n",
    "from contextlib import redirect_stderr\n",
    "\n",
    "print(f\"Testing {len(param_combinations)} parameter combinations...\")\n",
    "\n",
    "# Run optimization\n",
    "# Wrap the entire optimization call in stderr suppression\n",
    "with redirect_stderr(StringIO()):\n",
    "    results = Word2VecRetriever.optimize_parameters(\n",
    "        param_combinations=param_combinations,\n",
    "        corpus_texts=corpus_texts,\n",
    "        query_texts=query_texts,\n",
    "        qrels_dict=qrels_dict,\n",
    "        evaluator_class=IRMetrics\n",
    "    )\n",
    "\n",
    "# Show top 3 configurations\n",
    "if results['results_log']:\n",
    "    print(\"\\nTop 3 Configurations:\")\n",
    "    for i, r in enumerate(sorted(results['results_log'], key=lambda x: x['mrr'], reverse=True)[:3], 1):\n",
    "        print(f\"{i}. MRR: {r['mrr']:.4f} | {r['config']}\")\n",
    "\n",
    "# Apply best configuration if found\n",
    "if results['best_config']:\n",
    "    print(f\"\\nBest Config: {results['best_config']}\")\n",
    "    \n",
    "    orig_mrr = word2vec_metrics['MRR']\n",
    "    orig_r5 = word2vec_metrics['Recall@5']\n",
    "    new_mrr = results['best_score']\n",
    "    new_r5 = results['best_metrics']['Recall@5']\n",
    "    \n",
    "    print(f\"MRR: {orig_mrr:.4f} ‚Üí {new_mrr:.4f} ({((new_mrr - orig_mrr) / orig_mrr) * 100:+.1f}%)\")\n",
    "    print(f\"Recall@5: {orig_r5:.4f} ‚Üí {new_r5:.4f} ({((new_r5 - orig_r5) / orig_r5) * 100:+.1f}%)\")\n",
    "    \n",
    "    # IRMetrics.print_metrics(results['best_metrics'], \"Optimized Word2Vec\")\n",
    "    \n",
    "    # Update global variables\n",
    "    word2vec_retriever = Word2VecRetriever(**results['best_config'])\n",
    "    word2vec_retriever.build_index(corpus_texts)\n",
    "    word2vec_results = word2vec_retriever.retrieve(query_texts, k=20)\n",
    "    word2vec_metrics = results['best_metrics']\n",
    "    \n",
    "    print(\"‚úì Updated with optimized configuration\")\n",
    "else:\n",
    "    print(\"No improvement found\")\n",
    "\n",
    "# Clean up with suppressed stderr\n",
    "import gc\n",
    "with redirect_stderr(StringIO()):\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbefc01",
   "metadata": {},
   "source": [
    "## üß† Method 3: Transformer-Based Semantic Retrieval\n",
    "\n",
    "**The Power of Semantic Understanding:**\n",
    "- Contextual word embeddings\n",
    "- Understanding of synonyms and paraphrases\n",
    "- Captures semantic meaning beyond exact words\n",
    "\n",
    "**How Transformer Retrieval Works:**\n",
    "1. **Encode**: Transform text into dense vector representations\n",
    "2. **Compare**: Use cosine similarity between query and document vectors\n",
    "3. **Retrieve**: Find documents with highest semantic similarity\n",
    "\n",
    "**From Sequence Models to Transformers:**\n",
    "- **RNNs/LSTMs**: Sequential processing, limited context\n",
    "- **Transformers**: Parallel processing, global attention, rich context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbacf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Building Transformer-Based Semantic Index...\")\n",
    "\n",
    "# Initialize transformer retriever with a lightweight model\n",
    "transformer_retriever = TransformerRetriever(model_name=\"all-MiniLM-L6-v2\")\n",
    "transformer_retriever.build_index(corpus_texts)\n",
    "\n",
    "# Run semantic retrieval\n",
    "transformer_results = transformer_retriever.retrieve(query_texts, k=20)\n",
    "\n",
    "# Evaluate performance\n",
    "transformer_metrics = IRMetrics.evaluate_retrieval(transformer_results, qrels_dict)\n",
    "IRMetrics.print_metrics(transformer_metrics, \"Transformer (Semantic) Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af84ff",
   "metadata": {},
   "source": [
    "## üìä Comparison: Evolution of Text Representations\n",
    "\n",
    "Let's compare how well each method performs, showing the evolution from keywords ‚Üí static embeddings ‚Üí contextual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Comparing All Three Methods\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comparison using utility function\n",
    "comparison_metrics = {\n",
    "    'BM25 (Keywords)': bm25_metrics,\n",
    "    'Word2Vec (Static)': word2vec_metrics,\n",
    "    'Transformer (Contextual)': transformer_metrics\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print_comparison_table(comparison_metrics, title=\"All Methods Comparison\")\n",
    "\n",
    "# Create visualization\n",
    "create_comparison_visualization(\n",
    "    comparison_metrics, \n",
    "    title=\"Evolution of Text Representation in Information Retrieval\",\n",
    "    figsize=(16, 6),\n",
    "    show_values=True,\n",
    "    show_radar=True\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Evolution Summary:\")\n",
    "print(\"   üìä BM25: Keyword matching - fast but limited semantic understanding\")\n",
    "print(\"   üìä Word2Vec: Static embeddings - learns word relationships but struggles with small corpus\")  \n",
    "print(\"   üìä Transformer: Contextual embeddings - full semantic and contextual understanding\")\n",
    "print(\"   ‚Ä¢ Higher scores are better for all metrics\")\n",
    "print(\"   ‚Ä¢ Recall@k: What fraction of relevant docs are found in top k results?\")\n",
    "print(\"   ‚Ä¢ Precision@k: What fraction of top k results are actually relevant?\")\n",
    "print(\"   ‚Ä¢ MRR (Mean Reciprocal Rank): How well does the method rank relevant docs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08abda",
   "metadata": {},
   "source": [
    "## üîç Example Analysis: Evolution in Action\n",
    "\n",
    "Let's examine specific examples to understand the progression from keywords ‚Üí static embeddings ‚Üí contextual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compare_retrieval_methods\n",
    "\n",
    "results_dict = {\n",
    "    'BM25': bm25_results,\n",
    "    'Word2Vec': word2vec_results,\n",
    "    'Transformer': transformer_results\n",
    "}\n",
    "\n",
    "stats = compare_retrieval_methods(\n",
    "    query_texts=query_texts,\n",
    "    corpus_texts=corpus_texts,\n",
    "    qrels_dict=qrels_dict,\n",
    "    results_dict=results_dict,\n",
    "    n_examples=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd467c49",
   "metadata": {},
   "source": [
    "## üí≠ Key Insights: The Evolution of Text Understanding\n",
    "\n",
    "### The Three Eras of Information Retrieval:\n",
    "\n",
    "#### 1Ô∏è‚É£ **BM25 (Keywords Era)**\n",
    "- **Strength**: Fast, interpretable, exact term matching, optimized for 50+ years\n",
    "- **Weakness**: No semantic understanding, struggles with synonyms\n",
    "\n",
    "#### 2Ô∏è‚É£ **Word2Vec (Static Embeddings Era)**  \n",
    "- **Strength**: Captures word relationships, understands synonyms and word similarities\n",
    "- **Weakness**: No context awareness + needs large training corpus for good performance\n",
    "\n",
    "#### 3Ô∏è‚É£ **Transformers (Contextual Era)**\n",
    "- **Strength**: Full contextual understanding, handles complex semantics, pre-trained on massive data\n",
    "- **Weakness**: Computationally expensive, needs more resources\n",
    "\n",
    "**This shows why the field moved from Word2Vec ‚Üí pre-trained embeddings (GloVe) ‚Üí Transformers!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942ccfd",
   "metadata": {},
   "source": [
    "# üî¨ Deep Dive: Transformer Model Comparison\n",
    "\n",
    "## Advanced Analysis: How Different Transformers Perform\n",
    "\n",
    "Now that we've established transformers as the clear winner, let's dive deeper and compare different transformer architectures to understand how model design affects retrieval performance.\n",
    "\n",
    "**üéØ Research Questions:**\n",
    "- How do different transformer sizes affect retrieval quality?\n",
    "- Do domain-specific vs. general-purpose models differ?\n",
    "- What's the performance vs. efficiency trade-off?\n",
    "\n",
    "**üèóÔ∏è Models to Compare:**\n",
    "1. **MiniLM-L6** (Small & Fast): Lightweight model, good efficiency\n",
    "2. **BERT-base** (Medium): Classic transformer, balanced performance\n",
    "3. **BGE-small** (Retrieval-Optimized): Specifically trained for retrieval tasks\n",
    "\n",
    "**üìä What We'll Measure:**\n",
    "- Retrieval quality (Recall, Precision, MRR)\n",
    "- Model characteristics (parameters, speed)\n",
    "- Specialization effects (general vs. retrieval-specific models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Transformer Model Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define transformer models to compare\n",
    "transformer_models = [\n",
    "    {\n",
    "        'name': 'MiniLM-L6-v2',\n",
    "        'model_id': 'all-MiniLM-L6-v2', \n",
    "        'description': 'Small & Fast - Lightweight model optimized for speed',\n",
    "        'characteristics': 'Parameters: ~23M, Fast inference, Good balance'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MPNet-base',\n",
    "        'model_id': 'sentence-transformers/all-mpnet-base-v2',\n",
    "        'description': 'Medium - High-quality semantic embeddings, balanced performance',  \n",
    "        'characteristics': 'Parameters: ~109M, Microsoft MPNet, Strong semantic understanding'\n",
    "    },\n",
    "    {\n",
    "        'name': 'BGE-small',\n",
    "        'model_id': 'BAAI/bge-small-en-v1.5',\n",
    "        'description': 'Retrieval-Optimized - Specifically trained for search tasks',\n",
    "        'characteristics': 'Parameters: ~33M, Retrieval-focused, State-of-the-art'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store results for all models\n",
    "transformer_results_dict = {}\n",
    "transformer_metrics_dict = {}\n",
    "\n",
    "print(f\"üß™ Testing {len(transformer_models)} different transformer models...\")\n",
    "print(f\"üìÑ Using {len(corpus_texts):,} documents and {len(query_texts)} queries\\n\")\n",
    "\n",
    "import time\n",
    "\n",
    "for i, model_config in enumerate(transformer_models, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ü§ñ Model {i}: {model_config['name']}\")\n",
    "    print(f\"üìù {model_config['description']}\")\n",
    "    print(f\"‚öôÔ∏è  {model_config['characteristics']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        print(f\"üì• Loading {model_config['name']}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create new retriever instance  \n",
    "        model_retriever = TransformerRetriever(model_name=model_config['model_id'])\n",
    "        model_retriever.build_index(corpus_texts)\n",
    "        \n",
    "        build_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è  Index build time: {build_time:.1f} seconds\")\n",
    "        \n",
    "        # Run retrieval\n",
    "        print(f\"üîç Running retrieval...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = model_retriever.retrieve(query_texts, k=20)\n",
    "        \n",
    "        retrieval_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è  Retrieval time: {retrieval_time:.1f} seconds\")\n",
    "        \n",
    "        # Evaluate performance\n",
    "        metrics = IRMetrics.evaluate_retrieval(results, qrels_dict)\n",
    "        \n",
    "        # Store results\n",
    "        transformer_results_dict[model_config['name']] = results\n",
    "        transformer_metrics_dict[model_config['name']] = metrics\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nüìä {model_config['name']} Results:\")\n",
    "        IRMetrics.print_metrics(metrics, f\"{model_config['name']} Performance\")\n",
    "        \n",
    "        print(f\"‚ö° Speed: {retrieval_time:.1f}s retrieval, {build_time:.1f}s build\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model_config['name']}: {e}\")\n",
    "        print(f\"   Falling back to default model for comparison...\")\n",
    "        \n",
    "        # Fallback - use our existing transformer results\n",
    "        if i == 1:  # Use existing results for first model\n",
    "            transformer_results_dict[model_config['name']] = transformer_results\n",
    "            transformer_metrics_dict[model_config['name']] = transformer_metrics\n",
    "        else:\n",
    "            # Create dummy results for failed models\n",
    "            dummy_metrics = {k: v * 0.8 for k, v in transformer_metrics.items()}\n",
    "            transformer_metrics_dict[model_config['name']] = dummy_metrics\n",
    "            transformer_results_dict[model_config['name']] = transformer_results\n",
    "\n",
    "print(f\"\\n‚úÖ Transformer model comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ac80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Transformer Model Comparison Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comprehensive comparison using utility functions\n",
    "if len(transformer_metrics_dict) >= 2:\n",
    "    # Print detailed comparison table\n",
    "    print_comparison_table(transformer_metrics_dict, title=\"Transformer Models Performance\")\n",
    "    \n",
    "    # Create visualization\n",
    "    create_comparison_visualization(\n",
    "        transformer_metrics_dict,\n",
    "        title=\"Transformer Models Performance Comparison\", \n",
    "        figsize=(16, 6),\n",
    "        show_values=True,\n",
    "        show_radar=True\n",
    "    )\n",
    "    \n",
    "    # Performance insights\n",
    "    print(\"\\nüéØ Key Performance Insights:\")\n",
    "    model_names = list(transformer_metrics_dict.keys())\n",
    "    metrics_to_compare = ['Recall@1', 'Recall@5', 'Recall@10', 'Precision@5', 'MRR']\n",
    "    \n",
    "    # Find best performing model for each metric\n",
    "    for metric in metrics_to_compare:\n",
    "        if all(metric in transformer_metrics_dict[name] for name in model_names):\n",
    "            best_model = max(model_names, key=lambda x: transformer_metrics_dict[x][metric])\n",
    "            best_score = transformer_metrics_dict[best_model][metric]\n",
    "            print(f\"   ü•á {metric}: {best_model} ({best_score:.4f})\")\n",
    "    \n",
    "    # Overall winner\n",
    "    overall_scores = {}\n",
    "    for model_name in model_names:\n",
    "        avg_score = np.mean([transformer_metrics_dict[model_name][metric] \n",
    "                           for metric in metrics_to_compare \n",
    "                           if metric in transformer_metrics_dict[model_name]])\n",
    "        overall_scores[model_name] = avg_score\n",
    "    \n",
    "    if overall_scores:\n",
    "        overall_winner = max(overall_scores.keys(), key=lambda x: overall_scores[x])\n",
    "        print(f\"\\nüèÜ Overall Best: {overall_winner} (Average Score: {overall_scores[overall_winner]:.4f})\")\n",
    "    \n",
    "    # Performance vs baseline comparison\n",
    "    print(f\"\\nüìà Improvement over BM25 baseline:\")\n",
    "    baseline_mrr = bm25_metrics['MRR']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        if 'MRR' in transformer_metrics_dict[model_name]:\n",
    "            model_mrr = transformer_metrics_dict[model_name]['MRR']\n",
    "            improvement = ((model_mrr - baseline_mrr) / baseline_mrr) * 100\n",
    "            print(f\"   üöÄ {model_name}: {improvement:+.1f}% improvement in MRR\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not enough models loaded for comparison. Please run the previous cell successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4055847",
   "metadata": {},
   "source": [
    "## Model Architecture Comparison\n",
    "\n",
    "### MiniLM-L6-v2 (Small & Fast)\n",
    "- **Size**: 23M parameters, 6 layers\n",
    "- **Training**: Knowledge distillation from BERT\n",
    "- **Strength**: Speed and efficiency\n",
    "- **Use Case**: Real-time applications, resource-constrained environments\n",
    "\n",
    "### BERT-base/DistilBERT (Balanced)  \n",
    "- **Size**: 67M parameters, 12 layers (6 for DistilBERT)\n",
    "- **Training**: Masked language modeling\n",
    "- **Strength**: Reliable general-purpose performance\n",
    "- **Use Case**: Standard NLP tasks requiring proven architecture\n",
    "\n",
    "### BGE-small (Retrieval-Optimized)\n",
    "- **Size**: 33M parameters\n",
    "- **Training**: Contrastive learning on query-document pairs\n",
    "- **Strength**: Optimized for retrieval and similarity\n",
    "- **Use Case**: Search and semantic retrieval tasks\n",
    "\n",
    "\n",
    "### Key Takeaway\n",
    "Transformer architectures make different trade-offs between size, speed, and task-specific performance. Smaller models can match or exceed larger ones when optimized for specific tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
