{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6d7707",
   "metadata": {},
   "source": [
    "# Exercise: Custom Tokenizer and Vocabulary Builder\n",
    "\n",
    "**Estimated Time**: 15 minutes | **Status**: üöß Your implementation\n",
    "\n",
    "**Scenario**: Your chatbot splits \"can't\" into \"can\" + \"'t\" (losing negation) and \"USB-C\" into three tokens. Build a custom tokenizer that handles edge cases while keeping vocabulary manageable.\n",
    "\n",
    "**What You'll Learn**: Tokenization challenges, vocabulary building, encode/decode functionality, unknown word analysis, and why production systems use subword tokenization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc7f10",
   "metadata": {},
   "source": [
    "## ü§ñ Why This Matters in the GenAI Era\n",
    "\n",
    "**\"Doesn't ChatGPT handle all this already?\"** Great question! Here's why tokenization is MORE critical now:\n",
    "\n",
    "- **API Costs**: GPT-4 charges per token (~$0.03/1K). Better tokenization = lower costs\n",
    "- **Custom Models**: Fine-tuning, RAG systems, and domain-specific AI need proper tokenization\n",
    "- **Debugging AI**: When models fail, tokenization issues are often the root cause\n",
    "- **Performance**: Efficient tokenization directly impacts inference speed and memory usage\n",
    "\n",
    "**Bottom line**: Understanding tokenization helps you build better, faster, cheaper AI systems. Let's see how! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d451ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "PyTorch version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d1e7c",
   "metadata": {},
   "source": [
    "## Part A: Implement Basic Tokenizer\n",
    "\n",
    "Create a simple tokenizer that splits text into tokens and handles basic punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    \"\"\"\n",
    "    A custom tokenizer for handling text processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO - Initialize vocabulary dictionaries and special tokens\n",
    "        # Hint: You'll need word_to_id, id_to_word, vocab_counts\n",
    "        # Special tokens: PAD, UNK, START, END\n",
    "        pass\n",
    "    \n",
    "    def _add_word(self, word):\n",
    "        \"\"\"Add word to vocabulary if not present.\"\"\"\n",
    "        # TODO - Add word to both word_to_id and id_to_word mappings\n",
    "        pass\n",
    "    \n",
    "    def basic_tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Basic tokenization using whitespace splitting with punctuation handling.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Convert to lowercase for consistency\n",
    "        2. Split on whitespace\n",
    "        3. Handle punctuation separately\n",
    "        \"\"\"\n",
    "        # TODO - Implement tokenization logic\n",
    "        # Hint: Convert to lowercase, split on whitespace, handle punctuation\n",
    "        pass\n",
    "    \n",
    "    def test_edge_cases(self):\n",
    "        \"\"\"Test tokenizer on various examples.\"\"\"\n",
    "        test_cases = [\n",
    "            \"I can't help you today.\",           \n",
    "            \"My USB-C cable is broken.\",         \n",
    "            \"The customer's order was delayed.\", \n",
    "            \"It's a state-of-the-art system!\",  \n",
    "            \"Won't you help? That's odd.\",       \n",
    "        ]\n",
    "        \n",
    "        print(\"üß™ Testing tokenization:\\n\")\n",
    "        for i, text in enumerate(test_cases, 1):\n",
    "            tokens = self.basic_tokenize(text)\n",
    "            print(f\"{i}. Input: '{text}'\")\n",
    "            print(f\"   Tokens: {tokens}\")\n",
    "            print(f\"   Count: {len(tokens)} tokens\\n\")\n",
    "\n",
    "# Test the basic tokenizer\n",
    "# TODO - Create tokenizer instance and test it\n",
    "tokenizer = None  # Replace with your implementation\n",
    "# tokenizer.test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518136f",
   "metadata": {},
   "source": [
    "## Part B: Build a Frequency-Based Vocabulary\n",
    "\n",
    "Keep only frequently occurring tokens to manage vocabulary size and eliminate noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1136d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 1,000 customer support messages for vocabulary building\n",
    "print(\"üìä Loading customer support dataset...\")\n",
    "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", split=\"train\")\n",
    "\n",
    "# Extract 1,000 training messages and 200 test messages\n",
    "np.random.seed(42)\n",
    "all_indices = np.random.choice(len(dataset), size=1200, replace=False)\n",
    "train_indices = all_indices[:1000]\n",
    "test_indices = all_indices[1000:1200]\n",
    "\n",
    "train_messages = [dataset[int(idx)]['instruction'] for idx in train_indices]\n",
    "test_messages = [dataset[int(idx)]['instruction'] for idx in test_indices]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_messages)} training messages and {len(test_messages)} test messages\")\n",
    "print(f\"\\nSample training messages:\")\n",
    "for i, msg in enumerate(train_messages[:3], 1):\n",
    "    print(f\"{i}. {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdf654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(CustomTokenizer):  # Extend our existing class\n",
    "    def build_vocabulary(self, texts, min_frequency=5, max_vocab_size=10000):\n",
    "        \"\"\"\n",
    "        Build vocabulary from training texts with frequency-based filtering.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of training texts\n",
    "            min_frequency: Minimum token frequency to include in vocabulary\n",
    "            max_vocab_size: Maximum vocabulary size (keeps most frequent)\n",
    "        \"\"\"\n",
    "        print(f\"üèóÔ∏è Building vocabulary from {len(texts)} texts...\")\n",
    "        \n",
    "        # TODO - Count all tokens from all texts\n",
    "        # Hint: Loop through texts, tokenize each, update vocab_counts\n",
    "        \n",
    "        # TODO - Filter by frequency and limit vocabulary size\n",
    "        # Hint: Use vocab_counts.most_common(), filter by min_frequency\n",
    "        \n",
    "        # TODO - Add filtered tokens to vocabulary\n",
    "        # Hint: Use self._add_word() for each frequent token\n",
    "        \n",
    "        # TODO - Calculate and print statistics\n",
    "        # Show: total unique tokens, tokens in vocab, vocabulary reduction %\n",
    "        \n",
    "        return self.get_vocab_stats()\n",
    "    \n",
    "    def get_vocab_stats(self):\n",
    "        \"\"\"Get detailed vocabulary statistics.\"\"\"\n",
    "        # TODO - Calculate coverage percentage and other stats\n",
    "        # Return dictionary with vocab_size, total_unique_tokens, coverage_percentage\n",
    "        pass\n",
    "    \n",
    "    def show_most_frequent_tokens(self, top_n=20):\n",
    "        \"\"\"Display most frequent tokens in vocabulary.\"\"\"\n",
    "        # TODO - Show top N most frequent tokens with their frequencies\n",
    "        # Include whether each token is in the vocabulary or not\n",
    "        pass\n",
    "\n",
    "# Build vocabulary\n",
    "# TODO - Create new tokenizer, build vocabulary, show frequent tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba96992",
   "metadata": {},
   "source": [
    "## Part C: Convert Messages to Sequences\n",
    "\n",
    "Implement encode/decode to convert between text and numerical sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(CustomTokenizer):  # Extend our existing class\n",
    "    def encode(self, text, add_special_tokens=True, max_length=None):\n",
    "        \"\"\"\n",
    "        Convert text to sequence of token IDs.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to encode\n",
    "            add_special_tokens: Whether to add START/END tokens\n",
    "            max_length: Maximum sequence length (truncate if longer)\n",
    "            \n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        # TODO - Implement encoding logic\n",
    "        # 1. Tokenize text\n",
    "        # 2. Add special tokens if requested\n",
    "        # 3. Convert tokens to IDs (use UNK for unknown tokens)\n",
    "        # 4. Apply max_length truncation if specified\n",
    "        pass\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Convert sequence of token IDs back to text.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs\n",
    "            skip_special_tokens: Whether to exclude special tokens from output\n",
    "            \n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        # TODO - Implement decoding logic\n",
    "        # 1. Convert IDs to tokens\n",
    "        # 2. Remove special tokens if requested\n",
    "        # 3. Join tokens into text string\n",
    "        pass\n",
    "    \n",
    "    def test_round_trip(self, test_texts):\n",
    "        \"\"\"\n",
    "        Test encode/decode round-trip functionality.\n",
    "        \"\"\"\n",
    "        print(\"üîÑ Testing round-trip encoding/decoding:\\n\")\n",
    "        \n",
    "        for i, text in enumerate(test_texts, 1):\n",
    "            # TODO - Test encoding and decoding\n",
    "            # Show: original text, token IDs, unknown token count, decoded text\n",
    "            print(f\"{i}. Original: '{text}'\")\n",
    "            # Add your implementation here\n",
    "            print()\n",
    "\n",
    "# Test round-trip functionality\n",
    "test_examples = [\n",
    "    \"help with refund\",\n",
    "    \"I can't access my account\",\n",
    "    \"My USB-C cable is defective\",\n",
    "    \"The customer's order was cancelled\",\n",
    "    \"This is a new product called iPhone-15-Pro-Max\"  # Likely unknown words\n",
    "]\n",
    "\n",
    "# TODO - Test round-trip functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773565c7",
   "metadata": {},
   "source": [
    "## Part D: Analyze Limitations\n",
    "\n",
    "Understand what types of words our tokenizer misses and compare with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerAnalyzer:\n",
    "    def __init__(self, tokenizer, test_messages):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_messages = test_messages\n",
    "        self.unknown_tokens = Counter()\n",
    "        self.test_stats = {}\n",
    "    \n",
    "    def analyze_unknown_tokens(self):\n",
    "        \"\"\"\n",
    "        Analyze unknown token patterns in test set.\n",
    "        \"\"\"\n",
    "        print(\"üîç Analyzing unknown token patterns on test set...\\n\")\n",
    "        \n",
    "        # TODO - Analyze unknown tokens\n",
    "        # 1. Loop through test messages\n",
    "        # 2. Tokenize and encode each message\n",
    "        # 3. Count total tokens and unknown tokens\n",
    "        # 4. Track which specific tokens are unknown\n",
    "        \n",
    "        # TODO - Calculate and display statistics\n",
    "        # Show: total tokens, unknown tokens, unknown rate, unique unknown words\n",
    "        \n",
    "        return self.test_stats\n",
    "    \n",
    "    def show_most_common_unknown(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Show the most common unknown words.\n",
    "        \"\"\"\n",
    "        # TODO - Display most common unknown words with categories\n",
    "        pass\n",
    "    \n",
    "    def _categorize_unknown_word(self, word):\n",
    "        \"\"\"\n",
    "        Categorize unknown words by type.\n",
    "        \"\"\"\n",
    "        # TODO - Categorize words (Number/Code, Hyphenated, etc.)\n",
    "        pass\n",
    "    \n",
    "    def analyze_unknown_categories(self):\n",
    "        \"\"\"\n",
    "        Analyze what types of words are most commonly unknown.\n",
    "        \"\"\"\n",
    "        # TODO - Analyze categories of unknown words\n",
    "        pass\n",
    "\n",
    "# TODO - Run analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfcb72",
   "metadata": {},
   "source": [
    "## Comparison with Production Tokenizer\n",
    "\n",
    "Compare our custom tokenizer with BERT's subword approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_bert_tokenizer(custom_tokenizer, unknown_words, top_n=10):\n",
    "    \"\"\"\n",
    "    Compare how BERT handles our unknown words.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Comparing with BERT subword tokenizer...\\n\")\n",
    "    \n",
    "    # TODO - Load BERT tokenizer and compare\n",
    "    # 1. Load BERT tokenizer using AutoTokenizer\n",
    "    # 2. Compare vocabulary sizes\n",
    "    # 3. Test how BERT tokenizes our unknown words\n",
    "    # 4. Show examples of subword tokenization\n",
    "    \n",
    "    pass\n",
    "\n",
    "# TODO - Run comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75961a63",
   "metadata": {},
   "source": [
    "## Final Analysis & Visualization\n",
    "\n",
    "Visualize the vocabulary trade-offs and summarize key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_analysis_plot(custom_stats, test_stats, unknown_tokens):\n",
    "    \"\"\"\n",
    "    Create visualization of tokenizer analysis.\n",
    "    \"\"\"\n",
    "    # TODO - Create comprehensive visualization\n",
    "    # Create 2x2 subplot showing:\n",
    "    # 1. Vocabulary coverage pie chart\n",
    "    # 2. Vocabulary size reduction bar chart\n",
    "    # 3. Unknown word categories pie chart\n",
    "    # 4. Test set performance bar chart\n",
    "    \n",
    "    pass\n",
    "\n",
    "# TODO - Create visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5433d",
   "metadata": {},
   "source": [
    "## üéØ Key Discoveries\n",
    "\n",
    "**YOUR THOUGHTS**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
