{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Fine-Tune and Evaluate a Q&A Model\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Complete all **TODO** sections marked in the code. There are multiple TODOs total:\n",
    "- TODOs: In `exercise_utils_starter.py` (tokenization function)\n",
    "- TODOs: In `exercise_utils_starter.py` (metrics functions)\n",
    "- TODOs: In this notebook (training configuration)\n",
    "\n",
    "Follow the hints provided in comments!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper utilities (contains TODOs)\n",
    "import exercise_utils_starter as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fine-Tune DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This uses `prepare_train_features()` which contains **TODOs**. Make sure you've completed those first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Loading SQuAD 2.0...\")\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "print(\"\\nüîÑ Creating subsets...\")\n",
    "train_dataset = create_squad_subset(dataset['train'], n_samples=1000, seed=SEED)\n",
    "test_dataset = create_squad_subset(dataset['validation'], n_samples=200, seed=SEED)\n",
    "\n",
    "print(f\"\\n‚úÖ Ready: {len(train_dataset)} train, {len(test_dataset)} test\")\n",
    "\n",
    "example = train_dataset[0]\n",
    "print(f\"\\nExample Q: {example['question']}\")\n",
    "print(f\"Answer: {example['answers']['text'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_dataset = utils.get_tokenized_dataset(small_train_dataset, tokenizer, max_length=384)\n",
    "    print(f\"‚úì Training dataset prepared successfully! Size: {len(train_dataset)}\")\n",
    "    print(f\"  Sample keys: {list(train_dataset.features.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error in tokenization: {e}\")\n",
    "    print(\"   Check TODOs in exercise_utils_starter.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Dataset\n",
    "\n",
    "**Note:** This uses `prepare_train_features()` which contains **TODOs**. Make sure you've completed those first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Tokenizing...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: prepare_train_features(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    lambda x: prepare_validation_features(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Done: {len(tokenized_train)} train, {len(tokenized_test)} test samples\")\n",
    "\n",
    "# Verify tokenization\n",
    "print(\"\\nüîç Verifying...\")\n",
    "valid_answers = sum(1 for s in tokenized_train if s['start_positions'] > 0)\n",
    "print(f\"   Valid answers: {valid_answers}/{len(tokenized_train)}\")\n",
    "\n",
    "if valid_answers == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No valid answers found!\")\n",
    "    print(\"   Check TODOs in exercise_utils_starter.py\")\n",
    "elif valid_answers < len(tokenized_train) * 0.8:\n",
    "    print(f\"\\n‚ö†Ô∏è  Only {valid_answers/len(tokenized_train)*100:.0f}% valid\")\n",
    "    print(\"   Expected: >80%. Review tokenization logic.\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Good! {valid_answers/len(tokenized_train)*100:.0f}% have valid positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training\n",
    "\n",
    "### TODO: Set the learning rate\n",
    "**Hint:** Use a small learning rate appropriate for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(tokenized_train) // 16 * 3\n",
    "logging_steps = max(10, total_steps // 20)\n",
    "\n",
    "# TODO: Set learning_rate to 2e-5\n",
    "learning_rate = None  # TODO\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-distilbert-qa\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    logging_first_step=True,\n",
    "    seed=SEED,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(f\"‚öôÔ∏è  LR={training_args.learning_rate}, Batch=16, Epochs=3\")\n",
    "print(f\"   Logging every {logging_steps} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set learning_rate to 2e-5\n",
    "learning_rate = None  # Replace None with 2e-5\n",
    "\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Fine-tuning...\\n\")\n",
    "train_result = trainer.train()\n",
    "print(f\"\\n‚úÖ Done in {train_result.metrics['train_runtime']:.1f}s\")\n",
    "print(f\"   Final loss: {train_result.metrics.get('train_loss', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./finetuned-distilbert-qa\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"‚úÖ Saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluation\n",
    "\n",
    "**Note:** This section uses `compute_exact_match()` and `compute_f1_score()` which contain **TODOs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Baseline evaluation...\")\n",
    "\n",
    "baseline_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "baseline_trainer = Trainer(model=baseline_model, tokenizer=tokenizer)\n",
    "\n",
    "baseline_preds = baseline_trainer.predict(tokenized_test)\n",
    "baseline_answers = postprocess_qa_predictions(\n",
    "    test_dataset, tokenized_test,\n",
    "    (baseline_preds.predictions[0], baseline_preds.predictions[1])\n",
    ")\n",
    "\n",
    "references = {ex['id']: ex['answers']['text'] for ex in test_dataset}\n",
    "\n",
    "# Uses TODOs\n",
    "baseline_em = compute_exact_match(baseline_answers, references)\n",
    "baseline_f1 = compute_f1_score(baseline_answers, references)\n",
    "\n",
    "print(f\"\\nBaseline: EM={baseline_em:.1f}%, F1={baseline_f1:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Fine-tuned evaluation...\")\n",
    "\n",
    "finetuned_preds = trainer.predict(tokenized_test)\n",
    "finetuned_answers = postprocess_qa_predictions(\n",
    "    test_dataset, tokenized_test,\n",
    "    (finetuned_preds.predictions[0], finetuned_preds.predictions[1])\n",
    ")\n",
    "\n",
    "finetuned_em = compute_exact_match(finetuned_answers, references)\n",
    "finetuned_f1 = compute_f1_score(finetuned_answers, references)\n",
    "\n",
    "print(f\"\\nFine-tuned: EM={finetuned_em:.1f}%, F1={finetuned_f1:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_metrics = {'EM': baseline_em, 'F1': baseline_f1}\n",
    "finetuned_metrics = {'EM': finetuned_em, 'F1': finetuned_f1}\n",
    "\n",
    "print_metrics_summary(baseline_metrics, finetuned_metrics)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Baseline': baseline_metrics,\n",
    "    'Fine-Tuned': finetuned_metrics\n",
    "}).T\n",
    "df['EM Œî'] = df['EM'] - baseline_em\n",
    "df['F1 Œî'] = df['F1'] - baseline_f1\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_comparison({\n",
    "    'Baseline': baseline_metrics,\n",
    "    'Fine-Tuned': finetuned_metrics\n",
    "})\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° F1 improved by {finetuned_f1 - baseline_f1:.1f} points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Learning Rate Experiment\n",
    "\n",
    "### TODO: Set the higher learning rate\n",
    "**Hint:** Use a higher learning rate for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Higher LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Training with higher LR...\\n\")\n",
    "\n",
    "model_high_lr = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "\n",
    "total_steps_high = len(tokenized_train) // 16 * 2\n",
    "logging_steps_high = max(10, total_steps_high // 15)\n",
    "\n",
    "# TODO: Set higher_learning_rate to 5e-5\n",
    "higher_learning_rate = None  # TODO\n",
    "\n",
    "training_args_high = TrainingArguments(\n",
    "    output_dir=\"./finetuned-high-lr\",\n",
    "    learning_rate=higher_learning_rate,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=logging_steps_high,\n",
    "    logging_first_step=True,\n",
    "    seed=SEED,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer_high = Trainer(\n",
    "    model=model_high_lr,\n",
    "    args=training_args_high,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "train_result_high = trainer_high.train()\n",
    "print(f\"\\n‚úÖ Done in {train_result_high.metrics['train_runtime']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_high = trainer_high.predict(tokenized_test)\n",
    "answers_high = postprocess_qa_predictions(\n",
    "    test_dataset, tokenized_test,\n",
    "    (preds_high.predictions[0], preds_high.predictions[1])\n",
    ")\n",
    "\n",
    "em_high = compute_exact_match(answers_high, references)\n",
    "f1_high = compute_f1_score(answers_high, references)\n",
    "\n",
    "print(f\"High LR: EM={em_high:.1f}%, F1={f1_high:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'LR': ['2e-5', '5e-5'],\n",
    "    'Epochs': [3, 2],\n",
    "    'F1 (%)': [finetuned_f1, f1_high],\n",
    "    'EM (%)': [finetuned_em, em_high],\n",
    "    'Time (s)': [\n",
    "        train_result.metrics['train_runtime'],\n",
    "        train_result_high.metrics['train_runtime']\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "winner = '2e-5' if finetuned_f1 > f1_high else '5e-5'\n",
    "print(f\"\\nüèÜ Winner: LR={winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_rate_comparison([\n",
    "    {'lr': '2e-5', 'epochs': 3, 'em': finetuned_em, 'f1': finetuned_f1},\n",
    "    {'lr': '5e-5', 'epochs': 2, 'em': em_high, 'f1': f1_high}\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Reflection Questions\n",
    "\n",
    "### TODO: Answer these questions based on your results\n",
    "\n",
    "**Q1: Which learning rate performed better?**\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Why might a lower learning rate be preferred for fine-tuning? (Hint: catastrophic forgetting)**\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: What trade-off do you notice between learning rate and training time?**\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
