{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804c748a",
   "metadata": {},
   "source": [
    "# Lesson 4: Seq2Seq Models for Question Answering\n",
    "\n",
    "## Overview\n",
    "In this exercise, you'll build a seq2seq model that answers questions based on provided context. The model uses:\n",
    "- **Encoder**: Bidirectional LSTM that compresses context into a fixed-size vector (THE BOTTLENECK)\n",
    "- **Decoder**: Unidirectional LSTM that generates answers token-by-token\n",
    "- **Teacher Forcing**: During training, the model uses ground truth tokens instead of predictions\n",
    "\n",
    "The key insight: ALL context (whether 10 or 70 words) gets compressed into the same 256D vector. This creates a bottleneck that hurts performance on longer contexts. We'll measure this empirically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee94eb",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO: Import from your modules\n",
    "# from data import SyntheticQAGenerator, build_vocabulary, encode_text, decode_text\n",
    "# from models import Encoder, Decoder, Seq2Seq, count_parameters\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51817c",
   "metadata": {},
   "source": [
    "## Part A: Data Generation and Vocabulary\n",
    "\n",
    "### A.1: Generate Synthetic Q&A Data\n",
    "Create synthetic question-answer pairs with controlled context lengths to clearly demonstrate the bottleneck effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate synthetic Q&A data\n",
    "# Hint: \n",
    "# 1. Create a SyntheticQAGenerator instance\n",
    "# 2. Call generate_dataset() with counts for short (400), medium (400), and long (400) contexts\n",
    "# 3. Print the total number of examples and inspect one sample to understand the data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c14cd",
   "metadata": {},
   "source": [
    "### A.2: Build Vocabulary\n",
    "Create a vocabulary from all contexts, questions, and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f060562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build vocabulary from Q&A data\n",
    "# Hint:\n",
    "# 1. Use build_vocabulary() from your data module with the Q&A pairs\n",
    "# 2. Create a reverse mapping (idx2word) so you can decode token indices back to words\n",
    "# 3. Check the vocabulary size to understand how many unique words you have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326c25e",
   "metadata": {},
   "source": [
    "### A.3: Create PyTorch Dataset\n",
    "Encode Q&A pairs into token sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1681e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"Convert Q&A pairs to token sequences.\"\"\"\n",
    "    def __init__(self, qa_pairs, vocab, encode_fn):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.vocab = vocab\n",
    "        self.encode_fn = encode_fn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, question, answer = self.qa_pairs[idx]\n",
    "        \n",
    "        # TODO: Encode the data\n",
    "        # Hint: \n",
    "        # 1. Concatenate context and question with a separator token (e.g., \"[SEP]\")\n",
    "        # 2. Use encode_text() to convert both source (context+question) and target (answer) to token indices\n",
    "        # 3. Return the token sequences as torch tensors\n",
    "        \n",
    "        source_tokens = torch.zeros(20, dtype=torch.long)\n",
    "        target_tokens = torch.zeros(5, dtype=torch.long)\n",
    "        \n",
    "        return source_tokens, target_tokens\n",
    "\n",
    "# TODO: Split data and create DataLoaders\n",
    "# Hint:\n",
    "# 1. Split qa_data into train (70%), validation (20%), and test (10%) sets\n",
    "# 2. Create QADataset instances for each split\n",
    "# 3. Wrap each dataset in a DataLoader with appropriate batch size (try 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a824dd6",
   "metadata": {},
   "source": [
    "## Part B: Model Architecture\n",
    "\n",
    "### B.1: Initialize Model\n",
    "Create encoder, decoder, and seq2seq model. Make sure you've implemented `models.py` first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize your seq2seq model\n",
    "# Hint:\n",
    "# 1. Set embedding_dim (e.g., 128) and hidden_dim (256 is THE BOTTLENECK)\n",
    "# 2. Create Encoder, Decoder, and Seq2Seq instances using your vocabulary size\n",
    "# 3. Move model to device and print parameter count to verify setup\n",
    "# 4. Verify that the context vector size is 256D (understand why this is a bottleneck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95758bb",
   "metadata": {},
   "source": [
    "## Part C: Training\n",
    "\n",
    "### C.1: Setup Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc451fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup training components\n",
    "# Hint:\n",
    "# 1. Choose a loss function appropriate for classification (CrossEntropyLoss) and ignore padding tokens\n",
    "# 2. Choose an optimizer (Adam is a good choice) with a reasonable learning rate\n",
    "# 3. Understand what train_epoch() and evaluate() should do:\n",
    "#    - train_epoch: Use teacher forcing during training (decoder sees ground truth tokens)\n",
    "#    - evaluate: No teacher forcing during validation/test (decoder only sees its own predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070da4e",
   "metadata": {},
   "source": [
    "### C.2: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model\n",
    "# Hint:\n",
    "# 1. Run for ~30 epochs and track both training and validation losses\n",
    "# 2. Decay the teacher forcing ratio over time (start high, decrease gradually)\n",
    "# 3. Use train_epoch() for training and evaluate() for validation after each epoch\n",
    "# 4. Print metrics periodically to monitor progress (every 5 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ec049",
   "metadata": {},
   "source": [
    "### C.3: Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5240808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize training progress\n",
    "# Hint:\n",
    "# 1. Plot training and validation losses on the same graph\n",
    "# 2. Use proper labels, titles, and gridlines for clarity\n",
    "# 3. Observe: Does training loss decrease? Does validation loss converge or diverge?\n",
    "# 4. Look for signs of overfitting (training goes down but validation goes up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bd32c",
   "metadata": {},
   "source": [
    "## Part D: Inference and Bottleneck Analysis\n",
    "\n",
    "### D.1: Inference Function\n",
    "Generate answers for new context-question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement inference (answer generation)\n",
    "# Hint:\n",
    "# 1. Encode the context and question together (using your encode_text function)\n",
    "# 2. Pass the encoded input through the encoder to get the context vector (the bottleneck!)\n",
    "# 3. Use the context vector to seed the decoder\n",
    "# 4. Autoregressively generate tokens one at a time:\n",
    "#    - Take the current generated tokens as input\n",
    "#    - Decoder outputs logits for the next token\n",
    "#    - Select the token with highest probability (greedy decoding)\n",
    "#    - Stop when you generate an <END> token or reach max_length\n",
    "# 5. Decode the token indices back to text\n",
    "\n",
    "# Test on some examples from the test set and compare predictions to ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673db53",
   "metadata": {},
   "source": [
    "### D.2: Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b823c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate accuracy on test set\n",
    "# Hint:\n",
    "# 1. Iterate through the test set\n",
    "# 2. Generate predictions using your inference function\n",
    "# 3. Compare each prediction to the ground truth answer (exact match or case-insensitive)\n",
    "# 4. Calculate the percentage of correct predictions\n",
    "# 5. Print the overall test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece46a37",
   "metadata": {},
   "source": [
    "### D.3: Bottleneck Analysis\n",
    "\n",
    "**THE KEY INSIGHT**: The encoder compresses all contexts into the same 256D vector, regardless of context length. This should hurt performance significantly on longer contexts.\n",
    "\n",
    "Let's measure this empirically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e970130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: THE KEY EXERCISE - Analyze accuracy by context length\n",
    "# Hint:\n",
    "# 1. For each test example, collect: context_length (word count), prediction, ground_truth, is_correct\n",
    "# 2. Bucket results into three categories:\n",
    "#    - Short contexts: 8-12 words\n",
    "#    - Medium contexts: 25-35 words\n",
    "#    - Long contexts: 50-70 words\n",
    "# 3. Calculate accuracy within each bucket\n",
    "# 4. Plot a bar chart showing the three accuracies\n",
    "# 5. OBSERVE: You should see 30-40% degradation from short to long\n",
    "#    - Why? Because all contexts compress to THE SAME 256D vector!\n",
    "# 6. This empirically demonstrates why attention mechanisms are needed (next lesson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb104f9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **The Bottleneck Problem**: The encoder compresses all contexts into a fixed 256D vector. Whether the context is 10 words or 70 words, the model has the same amount of information to work with. This creates a fundamental limitation.\n",
    "\n",
    "2. **Empirical Evidence**: You should see ~30-40% accuracy degradation from short to long contexts:\n",
    "   - Short contexts (8-12 words): ~88-95% accuracy\n",
    "   - Medium contexts (25-35 words): ~75-80% accuracy  \n",
    "   - Long contexts (50-70 words): ~50-60% accuracy\n",
    "\n",
    "3. **Why Attention Matters**: The next lesson covers attention mechanisms, which allow the decoder to \"look back\" at the entire context during generation. Instead of compressing context into one vector, attention lets the model focus on relevant parts dynamically. This solves the bottleneck!\n",
    "\n",
    "### Key Takeaways\n",
    "- **Teacher Forcing**: Provides stable training by using ground truth tokens as input\n",
    "- **Bidirectional Encoder**: Captures context from both directions\n",
    "- **Context Vector**: The fixed-size bottleneck that limits model performance\n",
    "- **Attention (Next)**: Dynamic context focusing to overcome the bottleneck\n",
    "\n",
    "In Lesson 5, we'll add attention to eliminate this bottleneck and see performance improve significantly!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
