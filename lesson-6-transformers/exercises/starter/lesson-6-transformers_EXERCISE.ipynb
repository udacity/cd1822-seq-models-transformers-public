{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 Exercise: Visualize Attention Patterns in Pretrained Models\n",
    "\n",
    "**Estimated Time:** 18 minutes\n",
    "\n",
    "## Overview\n",
    "\n",
    "Dive deep into what pretrained Transformers have learned! You'll extract attention weights from multiple layers and heads, create visualization tools to interpret them, and discover that **different attention heads specialize in different linguistic phenomena** (syntax, coreference, semantic similarity).\n",
    "\n",
    "## Scenario\n",
    "\n",
    "Before deploying BERT for content moderation, your ML team wants to understand what it's actually learning. Visualize attention patterns to verify the model is focusing on contextually relevant words when detecting toxic language. This **interpretability analysis builds trust** in the model's decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Do\n",
    "\n",
    "**Part A:** Extract and Visualize Basic Attention (6 min)\n",
    "- Load BERT and tokenizer\n",
    "- Process example text with ambiguous \"bank\"\n",
    "- Examine positional encoding\n",
    "- Extract attention weights from all layers\n",
    "- Plot attention heatmap for Layer 6, Head 0\n",
    "\n",
    "**Part B:** Compare Multiple Attention Heads (6 min)\n",
    "- Visualize 4 different attention heads from same layer\n",
    "- Create 2Ã—2 subplot grid\n",
    "- Observe: Do different heads show different patterns?\n",
    "- Hypothesis: Some focus on nearby words, others on syntax\n",
    "\n",
    "**Part C:** Analyze Layer-by-Layer Attention (6 min)\n",
    "- Pick one attention head (Head 0)\n",
    "- Visualize across Layers 1, 4, 8, 12\n",
    "- Compare: Do early layers differ from late layers?\n",
    "- Discovery: Early = syntax, late = semantics\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import utilities\n",
    "from data import load_jigsaw_dataset\n",
    "from attention_utils import (\n",
    "    plot_attention_heatmap,\n",
    "    plot_multihead_attention,\n",
    "    plot_layer_progression,\n",
    "    extract_attention_patterns,\n",
    "    print_attention_patterns\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"\\nâœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Extract and Visualize Basic Attention (6 minutes)\n",
    "\n",
    "## Step 1: Load BERT and Tokenizer\n",
    "\n",
    "**TODO:** Load the pretrained BERT model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BERT model and tokenizer...\\n\")\n",
    "\n",
    "# TODO: Load BERT model and tokenizer\n",
    "# Hint: Use 'bert-base-uncased'\n",
    "# Hint: Set output_attentions=True to get attention weights\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# TODO: Load tokenizer\n",
    "# tokenizer = ???\n",
    "\n",
    "# TODO: Load model with output_attentions=True\n",
    "# model = ???\n",
    "\n",
    "# TODO: Move model to device and set to eval mode\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "print(\"âœ“ BERT loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Process Example Text\n",
    "\n",
    "We'll use the classic example: **\"The bank can refuse to lend money to the person by the river bank.\"**\n",
    "\n",
    "**TODO:** Tokenize the example and prepare inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading example text for attention visualization...\\n\")\n",
    "\n",
    "# TODO: Load the toxic comment dataset\n",
    "# Hint: Use load_jigsaw_dataset(n_samples=1000)\n",
    "# comments = ???\n",
    "\n",
    "# TODO: Select a good example for visualization\n",
    "# Hint: Find comments with good length (10-30 tokens) for attention visualization\n",
    "# good_examples = [c for c in comments if 10 < c['length'] < 30]\n",
    "# OR use this fallback example:\n",
    "example_text = \"The bank can refuse to lend money to the person by the river bank.\"\n",
    "\n",
    "print(\"Example Text:\")\n",
    "print(f\"  '{example_text}'\\n\")\n",
    "\n",
    "# TODO: Tokenize the text\n",
    "# Hint: Use tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "# inputs = ???\n",
    "\n",
    "# TODO: Move inputs to device\n",
    "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# TODO: Get token strings\n",
    "# Hint: Use tokenizer.convert_ids_to_tokens()\n",
    "# tokens = ???\n",
    "\n",
    "print(\"\\nTokenization:\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"  Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examine Position IDs\n",
    "\n",
    "**TODO:** Display the positional encoding to understand how Transformers know word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positional Encoding:\")\n",
    "\n",
    "# TODO: Extract or create position_ids\n",
    "# Hint: If 'position_ids' not in inputs, create with torch.arange()\n",
    "if 'position_ids' in inputs:\n",
    "    position_ids = inputs['position_ids'][0].tolist()\n",
    "else:\n",
    "    # TODO: Create position_ids\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    # position_ids = ???\n",
    "    pass\n",
    "\n",
    "print(f\"  Position IDs: {position_ids}\")\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"   These position IDs tell BERT the order of tokens.\")\n",
    "print(\"   Unlike RNNs, Transformers process all tokens in parallel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Attention Weights\n",
    "\n",
    "**TODO:** Run the model and extract attention weights from all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running model to extract attention weights...\\n\")\n",
    "\n",
    "# TODO: Run model forward pass\n",
    "# Hint: Use torch.no_grad() and model(**inputs)\n",
    "with torch.no_grad():\n",
    "    # outputs = ???\n",
    "    pass\n",
    "\n",
    "# TODO: Extract attention weights\n",
    "# Hint: outputs.attentions is a tuple of attention tensors\n",
    "# attention_weights = ???\n",
    "\n",
    "print(\"Attention Structure:\")\n",
    "print(f\"  Number of layers: {len(attention_weights)}\")\n",
    "print(f\"  Shape per layer: {attention_weights[0].shape}\")\n",
    "print(f\"  Format: (batch_size, n_heads, seq_len, seq_len)\")\n",
    "print(\"\\nâœ“ Attention weights extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plot Attention Heatmap for Layer 6, Head 0\n",
    "\n",
    "**TODO:** Visualize attention weights for a specific layer and head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Layer 6, Head 0\n",
    "layer_idx = 5  # 0-indexed, so layer 6 = index 5\n",
    "head_idx = 0\n",
    "\n",
    "print(f\"Visualizing Layer {layer_idx+1}, Head {head_idx}...\\n\")\n",
    "\n",
    "# TODO: Extract attention matrix for this layer and head\n",
    "# Hint: attention_weights[layer_idx][0] gives (n_heads, seq_len, seq_len)\n",
    "# Hint: Then select head_idx and convert to numpy\n",
    "# attn_layer = ???\n",
    "# attn_head = ???\n",
    "\n",
    "# TODO: Plot attention heatmap\n",
    "# Hint: Use plot_attention_heatmap() from attention_utils\n",
    "# fig = plot_attention_heatmap(\n",
    "#     attn_head,\n",
    "#     tokens,\n",
    "#     title=\"Your Attention Visualization\",\n",
    "#     layer=layer_idx+1,\n",
    "#     head=head_idx,\n",
    "#     figsize=(12, 10)\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Questions (Part A)\n",
    "\n",
    "Look at your attention heatmap and answer:\n",
    "\n",
    "1. **What patterns do you see?**\n",
    "   - Is there a diagonal pattern (self-attention)?\n",
    "   - Which tokens attend strongly to each other?\n",
    "\n",
    "2. **\"Bank\" disambiguation:**\n",
    "   - Where does the first \"bank\" (financial) attend?\n",
    "   - Where does the second \"bank\" (river) attend?\n",
    "   - Are they attending to different words?\n",
    "\n",
    "**TODO:** Write your observations (2-3 sentences):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANALYSIS HERE:**\n",
    "\n",
    "I observe that...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Compare Multiple Attention Heads (6 minutes)\n",
    "\n",
    "Now let's see if different attention heads learn different patterns!\n",
    "\n",
    "## TODO: Visualize 4 Different Heads\n",
    "\n",
    "Create a 2Ã—2 grid comparing Heads 0, 3, 7, and 11 from Layer 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 5  # Layer 6\n",
    "heads_to_compare = [0, 3, 7, 11]\n",
    "\n",
    "print(f\"Comparing {len(heads_to_compare)} attention heads from Layer {layer_idx+1}...\\n\")\n",
    "\n",
    "# TODO: Extract attention for all heads in this layer\n",
    "# Hint: attention_weights[layer_idx][0].cpu().numpy() gives (n_heads, seq_len, seq_len)\n",
    "# attn_layer = ???\n",
    "\n",
    "# TODO: Plot multi-head comparison\n",
    "# Hint: Use plot_multihead_attention() from attention_utils\n",
    "# fig = ???\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Questions (Part B)\n",
    "\n",
    "Compare the 4 attention heads and answer:\n",
    "\n",
    "1. **Do all heads show the same pattern?**\n",
    "   - Or do you see diversity?\n",
    "\n",
    "2. **Can you characterize each head?**\n",
    "   - Head 0: Local or long-range?\n",
    "   - Head 3: What relationships does it capture?\n",
    "   - Head 7: Semantic or syntactic?\n",
    "   - Head 11: Dense or sparse connections?\n",
    "\n",
    "3. **Hypothesis:**\n",
    "   - Do you think different heads specialize in different linguistic phenomena?\n",
    "   - What evidence supports this?\n",
    "\n",
    "**TODO:** Write your observations (3-4 sentences):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANALYSIS HERE:**\n",
    "\n",
    "Comparing the four heads, I notice...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Analyze Layer-by-Layer Attention (6 minutes)\n",
    "\n",
    "Do early layers learn different patterns than late layers?\n",
    "\n",
    "**Hypothesis:** Early layers capture syntax and local patterns, late layers capture semantics and long-range dependencies.\n",
    "\n",
    "## TODO: Visualize Head 0 Across Multiple Layers\n",
    "\n",
    "Compare Layers 1, 4, 8, and 12 (all using Head 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx = 0\n",
    "layers_to_compare = [0, 3, 7, 11]  # Layers 1, 4, 8, 12 (0-indexed)\n",
    "\n",
    "print(f\"Comparing Head {head_idx} across multiple layers...\\n\")\n",
    "\n",
    "# TODO: Extract attention weights for each layer (Head 0 only)\n",
    "# Hint: Create a list of attention matrices, one per layer\n",
    "# attention_by_layer = []\n",
    "# for layer_idx in range(len(attention_weights)):\n",
    "#     attn_layer = ??? \n",
    "#     attention_by_layer.append(attn_layer)\n",
    "\n",
    "# TODO: Plot layer progression\n",
    "# Hint: Use plot_layer_progression() from attention_utils\n",
    "# fig = ???\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Questions (Part C)\n",
    "\n",
    "Compare attention patterns across layers and answer:\n",
    "\n",
    "1. **Early layers (Layer 1, 4):**\n",
    "   - Do they focus on nearby tokens (local patterns)?\n",
    "   - Or long-range connections?\n",
    "   - What might they be capturing? (syntax? word order?)\n",
    "\n",
    "2. **Late layers (Layer 8, 12):**\n",
    "   - Do they show more diffuse attention?\n",
    "   - Do they connect distant tokens?\n",
    "   - What might they be capturing? (semantics? meaning?)\n",
    "\n",
    "3. **Hierarchical learning:**\n",
    "   - Does your observation support the hypothesis that Transformers learn hierarchically?\n",
    "   - Early = low-level patterns, Late = high-level meaning?\n",
    "\n",
    "**TODO:** Write your observations (3-4 sentences):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANALYSIS HERE:**\n",
    "\n",
    "Looking at the layer progression, I observe...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: What Did You Discover?\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "From your visualizations, you should have discovered:\n",
    "\n",
    "### 1. **Multi-Head Diversity**\n",
    "\n",
    "### 2. **Hierarchical Learning**\n",
    "\n",
    "### 3. **Contextualized Understanding**\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "**For Content Moderation:**\n",
    "- You can verify the model focuses on relevant words (not arbitrary patterns)\n",
    "- Interpretability builds trust in the system\n",
    "- Can identify and fix problematic attention patterns\n",
    "\n",
    "**For Deep Learning:**\n",
    "- Attention visualization reveals what models learn\n",
    "- Different layers capture different levels of abstraction\n",
    "- Multi-head attention provides multiple perspectives\n",
    "\n",
    "**For Modern NLP:**\n",
    "- This architecture powers GPT, BERT, ChatGPT, Claude\n",
    "- Understanding attention is key to understanding LLMs\n",
    "- Interpretability is critical for responsible AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
